{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome I am a Lecturer (Assistant Professor) in Finance and the Director of the Postgradaute Finance Programmes at the Adam Smith Business School, the University of Glasgow. Research Interest My research is focused on asset pricing, macroeconomics, more specifically, my research examines the interdependencies between financial and macroeconomic policies. My studies are motivated to deliver welfare gains in terms of financial and economic outcomes to our surrounding societies and driven towards policy. My research approach is based on a combination of data, applied theory, empirical methods and identification of the underlying economic drivers. My academic goals aspire to address real challenges via research and education. Contact You can find my contact information here .","title":"Home"},{"location":"#welcome","text":"I am a Lecturer (Assistant Professor) in Finance and the Director of the Postgradaute Finance Programmes at the Adam Smith Business School, the University of Glasgow.","title":"Welcome"},{"location":"#research-interest","text":"My research is focused on asset pricing, macroeconomics, more specifically, my research examines the interdependencies between financial and macroeconomic policies. My studies are motivated to deliver welfare gains in terms of financial and economic outcomes to our surrounding societies and driven towards policy. My research approach is based on a combination of data, applied theory, empirical methods and identification of the underlying economic drivers. My academic goals aspire to address real challenges via research and education.","title":"Research Interest"},{"location":"#contact","text":"You can find my contact information here .","title":"Contact"},{"location":"pages/contact/","text":"Contact Hormoz Ramian Email: hormoz . ramian @ glasgow . ac . uk Department Page Adam Smith Business School University of Glasgow University Avenue Glasgow G12 8QQ","title":"Contact"},{"location":"pages/contact/#contact","text":"Hormoz Ramian Email: hormoz . ramian @ glasgow . ac . uk Department Page Adam Smith Business School University of Glasgow University Avenue Glasgow G12 8QQ","title":"Contact"},{"location":"pages/cv/","text":"PDF Academic Positions Lecturer in Finance, Adam Smith Business School, University of Glasgow, 2020-present Postdoctoral Research Associate, Cambridge Judge Business School Academic Service Director, Finance Postgraduate Programmes (PGT), University of Glasgow Co-Convenor, Finance Postgraduate Programmes (PGT), University of Glasgow AACSB Accrediation Team, Adam Smith Business School, University of Glasgow CERFAS Executive Committee Member, Cambridge Judge Business School Finance Society, Imperial College London Student Investment Fund, Imperial College London Education Imperial College London (Finance, PhD) Imperial College London (MRes, Finance) University of Manchester (MSc, Financial Economics) Shahid Beheshti University (BSc, Economics)","title":"CV"},{"location":"pages/cv/#academic-positions","text":"Lecturer in Finance, Adam Smith Business School, University of Glasgow, 2020-present Postdoctoral Research Associate, Cambridge Judge Business School","title":"Academic Positions"},{"location":"pages/cv/#academic-service","text":"Director, Finance Postgraduate Programmes (PGT), University of Glasgow Co-Convenor, Finance Postgraduate Programmes (PGT), University of Glasgow AACSB Accrediation Team, Adam Smith Business School, University of Glasgow CERFAS Executive Committee Member, Cambridge Judge Business School Finance Society, Imperial College London Student Investment Fund, Imperial College London","title":"Academic Service"},{"location":"pages/cv/#education","text":"Imperial College London (Finance, PhD) Imperial College London (MRes, Finance) University of Manchester (MSc, Financial Economics) Shahid Beheshti University (BSc, Economics)","title":"Education"},{"location":"pages/more/","text":"Software Here's the list of packages I regularly use for managed data services and research: PyCharm Heoku Digital Ocean Github Repositories https://github.com/hormozramian/hramian.github.io - Datafeed routine for Stata - Export to web database - OCDB - MCMC - Latent Factor Autoregressive","title":"More"},{"location":"pages/more/#software","text":"Here's the list of packages I regularly use for managed data services and research: PyCharm Heoku Digital Ocean","title":"Software"},{"location":"pages/more/#github-repositories","text":"https://github.com/hormozramian/hramian.github.io - Datafeed routine for Stata - Export to web database - OCDB - MCMC - Latent Factor Autoregressive","title":"Github Repositories"},{"location":"pages/simulator/","text":"The following workshops are designed to apply financial pricing techniques to real-world scenarios. Each simulation embeds simple economic trade-offs into practical exercises. Students are assigned in groups learn how to incorporate simulated market information into trading strategies and execute them on the simulation platform below in a competitive environment. Bid-Ask The simulation provides an opportunity for students in finance, economics and related disciplines to develop a comprehensive insight over risk assessment and management on a market microstructure trading platform. Participants serve as market traders who are required to combine operations of financial markets with academic theories to maximize their investment value in an interactive and dynamic environment. More specifically, participants are tasked to discover a tradable asset\u2019s price based on their learnings from observed bid-ask prices posted by other market participants, compute their optimal bid-ask spread, and execute trades over multiple rounds. Participants face a trade-off between exploiting gains associated with posting profitable quotes based on their privately developed strategies versus minimizing losses associated with revealing their private information through their posted quoted prices. Schedule Monday 17 October 2022 9am (UK time) Timeline Briefing: 9:00-9:15am Practice Round: 9:15-9:30am Simulation Exercise: 9:30-11:30am Debriefing 11:45am Portal On the simulation day, use the Digital Ocean platform described below. Follow the instruction to access the trading platform via: http://159.89.95.249/login/ Study Material Handouts are distributed on the day. Participants are encouraged to review the Reading List in preparation for the trading simulation.","title":"Financial Simulation"},{"location":"pages/simulator/#bid-ask","text":"The simulation provides an opportunity for students in finance, economics and related disciplines to develop a comprehensive insight over risk assessment and management on a market microstructure trading platform. Participants serve as market traders who are required to combine operations of financial markets with academic theories to maximize their investment value in an interactive and dynamic environment. More specifically, participants are tasked to discover a tradable asset\u2019s price based on their learnings from observed bid-ask prices posted by other market participants, compute their optimal bid-ask spread, and execute trades over multiple rounds. Participants face a trade-off between exploiting gains associated with posting profitable quotes based on their privately developed strategies versus minimizing losses associated with revealing their private information through their posted quoted prices.","title":"Bid-Ask"},{"location":"pages/simulator/#schedule","text":"Monday 17 October 2022 9am (UK time)","title":"Schedule"},{"location":"pages/simulator/#timeline","text":"Briefing: 9:00-9:15am Practice Round: 9:15-9:30am Simulation Exercise: 9:30-11:30am Debriefing 11:45am","title":"Timeline"},{"location":"pages/simulator/#portal","text":"On the simulation day, use the Digital Ocean platform described below. Follow the instruction to access the trading platform via: http://159.89.95.249/login/","title":"Portal"},{"location":"pages/simulator/#study-material","text":"Handouts are distributed on the day. Participants are encouraged to review the Reading List in preparation for the trading simulation.","title":"Study Material"},{"location":"pages/Research/","text":"Working Papers Access to Credit and Short-Term Liquidity Sprint: Evidence from the European Labour Market (With Mario Cerrato and Shengfeng Mei, online appendix ) Abstract: Corporate credit lines have remained an indispensable source of short-term liquidity management, particularly across the European financial landscape. We provide novel empirical evidence that in the first quarter of 2022, the outstanding funds acquired via the credit lines by European companies reached \u20ac87bn, accounting for 5.15% of their total assets. Our study first provides a causal identification to explain corporate credit line drawdown decisions as a response to unexpected shortfalls in the realized earnings. We show that drawdowns increase by an average of 3.17 percentage points, measured in terms of credit line drawdowns scaled by the total assets, in response to a one percentage point (unanticipated) decline in the corporate earnings to their total assets. We further investigate the comprising components of company earning outcomes and show that the inelastic nature of labour in generating corporate earnings during the 2020:Q2 provides an alternative causal ident ification framework to trace exogenous shocks to the labour initially towards earning outcomes and to subsequently measure the corresponding variations on the drawdown decisions. The results remain consistent with the earlier findings, where the quantity of reliance on corporate credit lines is shown to be 3.34 percentage points given a change in the earning realizations for the companies with higher exposures to labour shocks while consistently establishing no credit drawdown result for companies with lower exposures to the shock. The diagram shows the estimated cross-sectional differential percentage point changes in the drawdown to total assets ratio given a one percentage point change in the EBITDA to total assets ratio during the pandemic. Each cross-sectional difference evaluates the corresponding shift in draw down decisions across the pairwise above- versus below-threshold value. The horizontal axis shows several bandwidth selections proportional to the standard deviation of the empirical distribution summarising EBITDA observations. Each value computed on the vertical axis is evaluated based on a separate estimation with an associated 95% confidence interval. The bandwidth selections considers even intervals around zero-earnings and shows a sharp shift in the firms\u2019 behaviours to draw down credit lines when facing marginally negative earnings while exhibiting no particular decision when facing marginally positive earnings. Optimal Financial Regulation: The interaction between monetary policy and capital regulation Abstract: I show that when the banking sector's assets comprise large excess reserves and loans, jointly determined capital regulation and interest-on-excess-reserves (IOER) policies provide welfare gains. In general equilibrium, falling IOER is associated with a proportional fall in deposit rate only when IOER is above the zero bound. This leads to a faster fall in the bank's interest expenses than its interest incomes. Given any lending level, lower net interest expenses enhance bank solvency. Nonetheless, the risk-weighted capital regulation remains unchanged and hence becomes socially costly. I show that jointly determined policies achieve welfare gains by loosening the capital requirement and lowering IOER to expand the credit flow, while bank failure likelihood remains constant. Conversely, lowering IOER below the zero bound is associated with a nonresponsive deposit rate that leads to growing net interest expenses and worsening bank solvency. In that case, I show that a stricter capital constraint together with a lower IOER provide social value. The social welfare increases towards the inner contour curves depicted by dashed (blue), dashed-dotted (orange) and solid (red) contours. The horizontal solid line describes the RW-capital regulation (I) that is decided in isolation of interest rate policy which is always associated with a lower welfare, relative to RW-capital regulation (II). Regulatory schedule (II) considers the welfare implications of lower IOER and is less strict than (I) and is able to achieve higher welfare relative to (I). Financial Regulation and Wealth Distribution Abstract: Financial regulation provides welfare gains to the society, at the expense of an exacerbated wealth distribution. I show that when capital markets are segmented, financial regulation leads to a transfer of wealth from depositors to equity investors. An integrated monetary and financial regulatory policy achieves welfare gains due to a credit flow expansion to the real sector, while default likelihood within the banking sector remains fixed. Nonetheless, this constrained equilibrium allocation is associated with lower deposit rate while dividends increase, leading to a wealth transfer across market segments. I provide sufficient conditions under which optimal financial regulation leads to welfare gains without exacerbating wealth heterogeneity. Pay Banks to Lend: Targeted Long-Term Refinancing Operations and the Fiscal Stimulus Abstract: The aftermath of the financial crisis inherited heightened economic uncertainty and low productivity. These features prompted the banking sectors across the developed economies to rely heavily on excess reserves offered by the central banks despite the negative nominal interest-on-excess-reserve (IOER) policy. Nonetheless, the negative relationship between the overall interest expenses of the banking sector with the IOER around the zero lower bound further exacerbates the over-reliance on excess reserves particularly when rates are negative. This paper shows that the new Targeted Long-Term Refinancing Operations (TLTRO) policy adopted by the central banks leads to expansionary effects when the refinancing lending rates fall below the IOER. I first provide a social welfare maximizing approach to determine the optimal borrowing limit. Second, I show that the policymaker's decision to finance the deficit due to remunerations depends on the trade-offs between the social gains associated with the expansion of lending to the real sector against the social costs of monetary tools (creating money to finance the gap) the fiscal stimulus. Joint Fiscal-Monetary Policy Responses to Transitory Aggregate Shocks I show that unilateral fiscal and monetary interventions, in response to transitory aggregate shocks, lead to welfare losses when real interest rates are low or possibly negative. A hyperactive fiscal policy in the forms of increased transfers and suppressed taxation revenues relies heavily on sovereign borrowing to smooth out adverse economic downturns, while the monetary base rate falls excessively to lower cost of finance. In general equilibrium with a banking sector subject to aggregate uncertainty, the transmission mechanism from the monetary policy to the credit sector weakens leading to an exacerbated real economic stagnation and heightened intermediary insolvencies. I show that a joint fiscal-monetary policy that optimally trades off higher credit flows to the real sector against lower rates delivers welfare gains during the transition and long-term by lowering future taxations to settle fiscal borrowing. Welfare implications of bank valuation disagreement - Insight - News & Insight - CJBS Negative interest rate: the interaction between monetary and financial regulatory policies - Insight - News & Insight - CJBS Current PhD Candidates Shengfeng Mei Working on credit lines and empirical banking Tongtong Wang Working on Trading Volume, Order Flow and Volatility in the Foreign Exchange Markets Arviansyah Putra Working on Central Counterparty Clearing","title":"Research"},{"location":"pages/Research/#working-papers","text":"Access to Credit and Short-Term Liquidity Sprint: Evidence from the European Labour Market (With Mario Cerrato and Shengfeng Mei, online appendix ) Abstract: Corporate credit lines have remained an indispensable source of short-term liquidity management, particularly across the European financial landscape. We provide novel empirical evidence that in the first quarter of 2022, the outstanding funds acquired via the credit lines by European companies reached \u20ac87bn, accounting for 5.15% of their total assets. Our study first provides a causal identification to explain corporate credit line drawdown decisions as a response to unexpected shortfalls in the realized earnings. We show that drawdowns increase by an average of 3.17 percentage points, measured in terms of credit line drawdowns scaled by the total assets, in response to a one percentage point (unanticipated) decline in the corporate earnings to their total assets. We further investigate the comprising components of company earning outcomes and show that the inelastic nature of labour in generating corporate earnings during the 2020:Q2 provides an alternative causal ident ification framework to trace exogenous shocks to the labour initially towards earning outcomes and to subsequently measure the corresponding variations on the drawdown decisions. The results remain consistent with the earlier findings, where the quantity of reliance on corporate credit lines is shown to be 3.34 percentage points given a change in the earning realizations for the companies with higher exposures to labour shocks while consistently establishing no credit drawdown result for companies with lower exposures to the shock.","title":"Working Papers"},{"location":"pages/Research/#current-phd-candidates","text":"Shengfeng Mei Working on credit lines and empirical banking Tongtong Wang Working on Trading Volume, Order Flow and Volatility in the Foreign Exchange Markets Arviansyah Putra Working on Central Counterparty Clearing","title":"Current PhD Candidates"},{"location":"pages/Teaching/","text":"Current Courses Data Science and Machine Learning in Finance ( ACCFIN5246 ) Financial Institutions, Intermediation and Regulation ( ACCFIN5014 ) Advanced Financial Modelling ( ACCFIN4085 ) Applied Statistical Computing (R) Undergraduate Dissertations I welcome ACCFIN4001P research proposals within the finance pathway. Please review the instructions page prior to preparing your research proposals. Previous Courses Financial Markets and Financial Institutions (ACCFIN4012) University of Glasgow, Adam Smith Business School Spring 2021, 2022 International Banking (ACCFIN5012) University of Glasgow, Adam Smith Business School Fall 2021, 2022","title":"Current Courses"},{"location":"pages/Teaching/#current-courses","text":"Data Science and Machine Learning in Finance ( ACCFIN5246 ) Financial Institutions, Intermediation and Regulation ( ACCFIN5014 ) Advanced Financial Modelling ( ACCFIN4085 ) Applied Statistical Computing (R)","title":"Current Courses"},{"location":"pages/Teaching/#undergraduate-dissertations","text":"I welcome ACCFIN4001P research proposals within the finance pathway. Please review the instructions page prior to preparing your research proposals.","title":"Undergraduate Dissertations"},{"location":"pages/Teaching/#previous-courses","text":"Financial Markets and Financial Institutions (ACCFIN4012) University of Glasgow, Adam Smith Business School Spring 2021, 2022 International Banking (ACCFIN5012) University of Glasgow, Adam Smith Business School Fall 2021, 2022","title":"Previous Courses"},{"location":"pages/Teaching/accfin4085/","text":"Overview The course covers essential techniques to formulate real-world financial problems into empirically testable models. This aim provides a foundation for the students to critically examine and re-express verbal problems into measurable quantities with a logical connection that proxies their real world relationships. The content focus on identifying information and observational biases embedded in data for the purpose of addressing financial problems. This aim provides a deeper insight into the empirical methods to distinguish between uncovering the underlying financial or economic behaviour embedded in the data versus spurious findings. Lastly the course shows how to develop financial modelling skills to inform decision-making based on financial market data, financial reports and stylised facts in empirical finance. This aim provides a basis to apply the empirical methods to several contexts and evaluate how data-driven approaches enhance predictions and performance. Course Contents The course is organised according to theories and empirical facts related to financial markets and institutions. Both aspects are essential in terms of understanding the course material and examinations but also in terms of their importance towards developing a foundation for future careers in finance within or outside academia. Financial markets and financial institutions is delivered across the following main units: Examine the modern empirical models in finance and assess their performance in informing financial decisions. Translate conceptual or verbal theories into empirically testable financial models. Develop software routines to implement statistical estimation and inference methods to quantify modelling outcomes. Apply analytical and computational techniques to identify the observational and statistical biases within the context of empirical finance. Apply the longitudinal data analysis to address the implications of observational and statistical biases within empirical finance. Examine modern learning-based frameworks and inform model selection. Course Timetable The course is delivered via weekly sessions and six tutorial workshops. There are practice problem sets with solutions to further illustrate theories and implementations. There are three assessment assignments through the semester timetable below. The timetable below is subject to change, please review this timetable on weekly basis: Course Schedule Office Hours Friday 10-11 am, Gilbert Scott Building Course Tutorials and GTA Support You are expected to have covered the material ahead of the tutorials. Tongtong Wang holds weekly office hours, starting in week 1. The schedule will be posted on MyGlasgow . Financial Datasets and Empirical Exercises The course contents, practice problem sets and assessment components are based on real-world financial data. Therefore, it is a requirement that all class participants set up their accounts with the data platforms described below: Register your accounts on Financial Analysis Made Easy (FAME) via the university library and additionally Wharton Research Data Services directly on their platform using the university email address. This registration is then activated by the business database administration within one week. Please initiate the registration in the first week of the course before we progress towards further course contents and assignments. Key statistics and learning outcomes arising from the activities related to the data will be part of the exam. Treat the empirical exercises as an essential part of the learning experience As a financial analyst or a research financial economist, you will work with the very same data providers repeatedly. Developing an understanding of the empirical counterparts of theories will be an important takeaway for future careers in finance. Problem Sets Problem Set 1 Problem Set 2 Problem Set 3 Assessments Individual Project Presentation (25%): The project is proposed based on one of the course methodologies to examine a context of interest. The written project is expected to outline the significance of the topic and demonstrate how the main findings are quantified based on the statistical evidence and methods. The projects are presented during a 15 minutes slot followed by Q&A. Degree exam in April/May (75%): The final assessment will be based on the written report presented earlier and fully developed to examine the research question, methodologies and a comprehensive interpretation of the results. Grading Grading is based on meeting the course intended learning outcomes examined in each assignment and following the University's Schedule A . Grades are rewarded based on both the input and output presented in each part thus demonstrating intermediate steps building up towards an overall answer are required and graded. Problem set and assignments require accessing real-world financial data from the professional platforms, thus class participants are required to register and activate their accounts with data providers by following the information provided. Feedback Answers to the assignments will be provided in the subsequent week after the deadline and after everyone's submissions are received. Aside from the assessed assignments indicated above, the course includes two practice problem sets with solutions. These are distributed to practice theories and implementations during the semester. Students are expected to attend the office hours and tutorial workshops for reviewing specific queries. Past Papers Past exam papers are available via the university portal . These can serve as a basis for preparation, however, note that the exam and course contents are subject to changes on an annual basis. Textbook and Reading List Financial Decisions and Markets, John Campbell, 2017 Edition [MT-1] Introduction to Banking, Casu, Girardone and Molyneux, 2nd or 3rd Edition [MT-2] The Economics of Money, Banking and Financial Markets (Frederic Mishkin) [OT-1] Further to the textbooks, there will be journal article readings cited throughout the course. Journal articles indicated as 'required reading' should also be studied in conjunction with textbook reading and form part of the assessments: Reading List . There are additional articles from the Financial Times to complement and relate to the ongoing economic and financial outcomes.","title":"Advanced Financial Modelling"},{"location":"pages/Teaching/accfin4085/#overview","text":"The course covers essential techniques to formulate real-world financial problems into empirically testable models. This aim provides a foundation for the students to critically examine and re-express verbal problems into measurable quantities with a logical connection that proxies their real world relationships. The content focus on identifying information and observational biases embedded in data for the purpose of addressing financial problems. This aim provides a deeper insight into the empirical methods to distinguish between uncovering the underlying financial or economic behaviour embedded in the data versus spurious findings. Lastly the course shows how to develop financial modelling skills to inform decision-making based on financial market data, financial reports and stylised facts in empirical finance. This aim provides a basis to apply the empirical methods to several contexts and evaluate how data-driven approaches enhance predictions and performance.","title":"Overview"},{"location":"pages/Teaching/accfin4085/#course-contents","text":"The course is organised according to theories and empirical facts related to financial markets and institutions. Both aspects are essential in terms of understanding the course material and examinations but also in terms of their importance towards developing a foundation for future careers in finance within or outside academia. Financial markets and financial institutions is delivered across the following main units: Examine the modern empirical models in finance and assess their performance in informing financial decisions. Translate conceptual or verbal theories into empirically testable financial models. Develop software routines to implement statistical estimation and inference methods to quantify modelling outcomes. Apply analytical and computational techniques to identify the observational and statistical biases within the context of empirical finance. Apply the longitudinal data analysis to address the implications of observational and statistical biases within empirical finance. Examine modern learning-based frameworks and inform model selection.","title":"Course Contents"},{"location":"pages/Teaching/accfin4085/#course-timetable","text":"The course is delivered via weekly sessions and six tutorial workshops. There are practice problem sets with solutions to further illustrate theories and implementations. There are three assessment assignments through the semester timetable below. The timetable below is subject to change, please review this timetable on weekly basis: Course Schedule","title":"Course Timetable"},{"location":"pages/Teaching/accfin4085/#office-hours","text":"Friday 10-11 am, Gilbert Scott Building","title":"Office Hours"},{"location":"pages/Teaching/accfin4085/#course-tutorials-and-gta-support","text":"You are expected to have covered the material ahead of the tutorials. Tongtong Wang holds weekly office hours, starting in week 1. The schedule will be posted on MyGlasgow .","title":"Course Tutorials and GTA Support"},{"location":"pages/Teaching/accfin4085/#financial-datasets-and-empirical-exercises","text":"The course contents, practice problem sets and assessment components are based on real-world financial data. Therefore, it is a requirement that all class participants set up their accounts with the data platforms described below: Register your accounts on Financial Analysis Made Easy (FAME) via the university library and additionally Wharton Research Data Services directly on their platform using the university email address. This registration is then activated by the business database administration within one week. Please initiate the registration in the first week of the course before we progress towards further course contents and assignments. Key statistics and learning outcomes arising from the activities related to the data will be part of the exam. Treat the empirical exercises as an essential part of the learning experience As a financial analyst or a research financial economist, you will work with the very same data providers repeatedly. Developing an understanding of the empirical counterparts of theories will be an important takeaway for future careers in finance.","title":"Financial Datasets and Empirical Exercises"},{"location":"pages/Teaching/accfin4085/#problem-sets","text":"Problem Set 1 Problem Set 2 Problem Set 3","title":"Problem Sets"},{"location":"pages/Teaching/accfin4085/#assessments","text":"Individual Project Presentation (25%): The project is proposed based on one of the course methodologies to examine a context of interest. The written project is expected to outline the significance of the topic and demonstrate how the main findings are quantified based on the statistical evidence and methods. The projects are presented during a 15 minutes slot followed by Q&A. Degree exam in April/May (75%): The final assessment will be based on the written report presented earlier and fully developed to examine the research question, methodologies and a comprehensive interpretation of the results.","title":"Assessments"},{"location":"pages/Teaching/accfin4085/#grading","text":"Grading is based on meeting the course intended learning outcomes examined in each assignment and following the University's Schedule A . Grades are rewarded based on both the input and output presented in each part thus demonstrating intermediate steps building up towards an overall answer are required and graded. Problem set and assignments require accessing real-world financial data from the professional platforms, thus class participants are required to register and activate their accounts with data providers by following the information provided.","title":"Grading"},{"location":"pages/Teaching/accfin4085/#feedback","text":"Answers to the assignments will be provided in the subsequent week after the deadline and after everyone's submissions are received. Aside from the assessed assignments indicated above, the course includes two practice problem sets with solutions. These are distributed to practice theories and implementations during the semester. Students are expected to attend the office hours and tutorial workshops for reviewing specific queries.","title":"Feedback"},{"location":"pages/Teaching/accfin4085/#past-papers","text":"Past exam papers are available via the university portal . These can serve as a basis for preparation, however, note that the exam and course contents are subject to changes on an annual basis.","title":"Past Papers"},{"location":"pages/Teaching/accfin4085/#textbook-and-reading-list","text":"Financial Decisions and Markets, John Campbell, 2017 Edition [MT-1] Introduction to Banking, Casu, Girardone and Molyneux, 2nd or 3rd Edition [MT-2] The Economics of Money, Banking and Financial Markets (Frederic Mishkin) [OT-1] Further to the textbooks, there will be journal article readings cited throughout the course. Journal articles indicated as 'required reading' should also be studied in conjunction with textbook reading and form part of the assessments: Reading List . There are additional articles from the Financial Times to complement and relate to the ongoing economic and financial outcomes.","title":"Textbook and Reading List"},{"location":"pages/Teaching/accfin5014/","text":"Overview Financial system includes individual and institutional investors, markets and intermediaries, ultimate borrowers, and lastly the government which supervises and regulates the markets and institutions. The main role of a financial system is to channel funds from the savers to borrowers and facilitate investment and funding decisions. Investment decisions involve many layers, such as the time value of money and risk associated with investments\u2019 potential payoffs. Risk pricing is a quintessential feature of a wider-ranging family of financial investments. The course examines the information that investors need to consider to price financial assets including potential payoffs and likelihoods of the potential payoffs. Depending on each investor\u2019s attitude towards risk, such considerations lead to different valuations of the same investment. The course continue by defining the various types of financial intermediaries, their typical balance sheet and risks they face. We then describe a framework that relies on our learning from financial returns and construction of certainty equivalent to quantify the amount of liquidity provided by the intermediation sector across the financial system and discuss the implication of decisions made at the intermediation sector on availability of credit and shareholder valuation. Course Contents The course is organised according to theories and empirical facts related to financial markets and institutions. Both aspects are essential in terms of understanding the course material and examinations but also in terms of their importance towards developing a foundation for future careers in finance within or outside academia. Financial markets and financial institutions is delivered across the following main units: Asset Pricing: Micro-foundations Asset Pricing: Macro-foundations Financial Markets Financial Intermediaries 1: Depository Institutions Financial Intermediaries 2: Venture Capitals and Early Stage Financing International Banking Financial Crisis Central Bank and Monetary Policy Ethics and Financial Misconduct Financial Regulation Course Timetable The course is delivered via weekly sessions and six tutorial workshops. There are practice problem sets with solutions to further illustrate theories and implementations. There are three assessment assignments through the semester timetable below. The timetable below is subject to change, please review this timetable on weekly basis: Course Schedule Office Hours Friday 10-11 am, Gilbert Scott Building Course Tutorials and GTA Support You are expected to have covered the material ahead of the tutorials. Tongtong Wang holds weekly office hours, starting in week 1. The schedule will be posted on MyGlasgow . Financial Datasets and Empirical Exercises The course contents, practice problem sets and assessment components are based on real-world financial data. Therefore, it is a requirement that all class participants set up their accounts with the data platforms described below: Register your accounts on Financial Analysis Made Easy (FAME) via the university library and additionally Wharton Research Data Services directly on their platform using the university email address. This registration is then activated by the business database administration within one week. Please initiate the registration in the first week of the course before we progress towards further course contents and assignments. Key statistics and learning outcomes arising from the activities related to the data will be part of the exam. Treat the empirical exercises as an essential part of the learning experience As a financial analyst or a research financial economist, you will work with the very same data providers repeatedly. Developing an understanding of the empirical counterparts of theories will be an important takeaway for future careers in finance. Problem Sets Problem Set 1 Problem Set 2 Problem Set 3 Assessments Individual Assignment (25%) includes a problem sheet requiring methodological derivations, numerical computations followed by interpretation of results. The problem sheet will be distributed on 31 January to access and will be due on 15 February (deadline). Group Assignment (25%) includes a problem sheet requiring methodological derivations, numerical computations followed by interpretation of results. The problem sheet will be distributed early March to access and will be due on 16 March (deadline). Degree exam in April/May (50%): The final exam will be an individual assessment covering all course contents during the semester including key facts and statistics arising from empirical exercises, methodological derivations and computations. Information regarding the final examination will be released towards the end of the semester. Grading Grading is based on meeting the course intended learning outcomes examined in each assignment and following the University's Schedule A . Grades are rewarded based on both the input and output presented in each part thus demonstrating intermediate steps building up towards an overall answer are required and graded. Problem set and assignments require accessing real-world financial data from the professional platforms, thus class participants are required to register and activate their accounts with data providers by following the information provided. Feedback Answers to the assignments will be provided in the subsequent week after the deadline and after everyone's submissions are received. Aside from the assessed assignments indicated above, the course includes two practice problem sets with solutions. These are distributed to practice theories and implementations during the semester. Students are expected to attend the office hours and tutorial workshops for reviewing specific queries. Past Papers Past exam papers are available via the university portal . These can serve as a basis for preparation, however, note that the exam and course contents are subject to changes on an annual basis. Textbook and Reading List Financial Decisions and Markets, John Campbell, 2017 Edition [MT-1] Introduction to Banking, Casu, Girardone and Molyneux, 2nd or 3rd Edition [MT-2] The Economics of Money, Banking and Financial Markets (Frederic Mishkin) [OT-1] Further to the textbooks, there will be journal article readings cited throughout the course. Journal articles indicated as 'required reading' should also be studied in conjunction with textbook reading and form part of the assessments: Reading List . There are additional articles from the Financial Times to complement and relate to the ongoing economic and financial outcomes.","title":"Financial Institutions, Intermediation and Regulation"},{"location":"pages/Teaching/accfin5014/#overview","text":"Financial system includes individual and institutional investors, markets and intermediaries, ultimate borrowers, and lastly the government which supervises and regulates the markets and institutions. The main role of a financial system is to channel funds from the savers to borrowers and facilitate investment and funding decisions. Investment decisions involve many layers, such as the time value of money and risk associated with investments\u2019 potential payoffs. Risk pricing is a quintessential feature of a wider-ranging family of financial investments. The course examines the information that investors need to consider to price financial assets including potential payoffs and likelihoods of the potential payoffs. Depending on each investor\u2019s attitude towards risk, such considerations lead to different valuations of the same investment. The course continue by defining the various types of financial intermediaries, their typical balance sheet and risks they face. We then describe a framework that relies on our learning from financial returns and construction of certainty equivalent to quantify the amount of liquidity provided by the intermediation sector across the financial system and discuss the implication of decisions made at the intermediation sector on availability of credit and shareholder valuation.","title":"Overview"},{"location":"pages/Teaching/accfin5014/#course-contents","text":"The course is organised according to theories and empirical facts related to financial markets and institutions. Both aspects are essential in terms of understanding the course material and examinations but also in terms of their importance towards developing a foundation for future careers in finance within or outside academia. Financial markets and financial institutions is delivered across the following main units: Asset Pricing: Micro-foundations Asset Pricing: Macro-foundations Financial Markets Financial Intermediaries 1: Depository Institutions Financial Intermediaries 2: Venture Capitals and Early Stage Financing International Banking Financial Crisis Central Bank and Monetary Policy Ethics and Financial Misconduct Financial Regulation","title":"Course Contents"},{"location":"pages/Teaching/accfin5014/#course-timetable","text":"The course is delivered via weekly sessions and six tutorial workshops. There are practice problem sets with solutions to further illustrate theories and implementations. There are three assessment assignments through the semester timetable below. The timetable below is subject to change, please review this timetable on weekly basis: Course Schedule","title":"Course Timetable"},{"location":"pages/Teaching/accfin5014/#office-hours","text":"Friday 10-11 am, Gilbert Scott Building","title":"Office Hours"},{"location":"pages/Teaching/accfin5014/#course-tutorials-and-gta-support","text":"You are expected to have covered the material ahead of the tutorials. Tongtong Wang holds weekly office hours, starting in week 1. The schedule will be posted on MyGlasgow .","title":"Course Tutorials and GTA Support"},{"location":"pages/Teaching/accfin5014/#financial-datasets-and-empirical-exercises","text":"The course contents, practice problem sets and assessment components are based on real-world financial data. Therefore, it is a requirement that all class participants set up their accounts with the data platforms described below: Register your accounts on Financial Analysis Made Easy (FAME) via the university library and additionally Wharton Research Data Services directly on their platform using the university email address. This registration is then activated by the business database administration within one week. Please initiate the registration in the first week of the course before we progress towards further course contents and assignments. Key statistics and learning outcomes arising from the activities related to the data will be part of the exam. Treat the empirical exercises as an essential part of the learning experience As a financial analyst or a research financial economist, you will work with the very same data providers repeatedly. Developing an understanding of the empirical counterparts of theories will be an important takeaway for future careers in finance.","title":"Financial Datasets and Empirical Exercises"},{"location":"pages/Teaching/accfin5014/#problem-sets","text":"Problem Set 1 Problem Set 2 Problem Set 3","title":"Problem Sets"},{"location":"pages/Teaching/accfin5014/#assessments","text":"Individual Assignment (25%) includes a problem sheet requiring methodological derivations, numerical computations followed by interpretation of results. The problem sheet will be distributed on 31 January to access and will be due on 15 February (deadline). Group Assignment (25%) includes a problem sheet requiring methodological derivations, numerical computations followed by interpretation of results. The problem sheet will be distributed early March to access and will be due on 16 March (deadline). Degree exam in April/May (50%): The final exam will be an individual assessment covering all course contents during the semester including key facts and statistics arising from empirical exercises, methodological derivations and computations. Information regarding the final examination will be released towards the end of the semester.","title":"Assessments"},{"location":"pages/Teaching/accfin5014/#grading","text":"Grading is based on meeting the course intended learning outcomes examined in each assignment and following the University's Schedule A . Grades are rewarded based on both the input and output presented in each part thus demonstrating intermediate steps building up towards an overall answer are required and graded. Problem set and assignments require accessing real-world financial data from the professional platforms, thus class participants are required to register and activate their accounts with data providers by following the information provided.","title":"Grading"},{"location":"pages/Teaching/accfin5014/#feedback","text":"Answers to the assignments will be provided in the subsequent week after the deadline and after everyone's submissions are received. Aside from the assessed assignments indicated above, the course includes two practice problem sets with solutions. These are distributed to practice theories and implementations during the semester. Students are expected to attend the office hours and tutorial workshops for reviewing specific queries.","title":"Feedback"},{"location":"pages/Teaching/accfin5014/#past-papers","text":"Past exam papers are available via the university portal . These can serve as a basis for preparation, however, note that the exam and course contents are subject to changes on an annual basis.","title":"Past Papers"},{"location":"pages/Teaching/accfin5014/#textbook-and-reading-list","text":"Financial Decisions and Markets, John Campbell, 2017 Edition [MT-1] Introduction to Banking, Casu, Girardone and Molyneux, 2nd or 3rd Edition [MT-2] The Economics of Money, Banking and Financial Markets (Frederic Mishkin) [OT-1] Further to the textbooks, there will be journal article readings cited throughout the course. Journal articles indicated as 'required reading' should also be studied in conjunction with textbook reading and form part of the assessments: Reading List . There are additional articles from the Financial Times to complement and relate to the ongoing economic and financial outcomes.","title":"Textbook and Reading List"},{"location":"pages/Teaching/accfin5246/","text":"Course Syllabus PDF Overview Addressing real-world economic and financial problems via information embedded in data is an active area of academic and professional interests. This course contributes towards this goal with the following two approaches. First, the course provides a foundation to methodically structure large-dimensional datasets and summarise information into interpretable outcomes. The second part of the course examines how the combination of large datasets accompanied by statistical learning and artificial intelligence techniques are helping practitioners to make more efficient economic and financial decisions. The cwourse is delivered based on a balanced combination of (1) descriptive contents required to formulate financial and economic problems into quantifiable objects of interest, (2) analytical derivations and statistical techniques and (3) software programming. The course is structured based on four pillars. All pillars are equally weighted in terms of course contents and examinations but also in terms of their importance towards developing a foundation for future careers in data science and machine learning in finance within or outside academia: Data Methodological Frameworks Software Implementations Finance Theory and Applications Course Contents Data Science: Theories and Implementations Regression Analysis: Theory Regression Analysis: Implementations and Applications Machine Learning: Theory Machine Learning: Implementations and Applications Prerequisites There is no formal prerequisite, however a prior background including calculus, statistics and regression is favourable. The course assumes familiarity with the estimation and inference of the least squares framework covered earlier in the semester one courses. Course Timetable The course is delivered via weekly sessions and four tutorial workshops. There are three practice problem sets with solutions to further illustrate theories and implementations, followed by three assessment assignments outlined in the semester timetable below. The timetable below is subject to change, please review this timetable on weekly basis: Course Schedule Office Hours Friday 9-10 am (Gilbert Scott Building) Course Tutorials and GTA Support You are expected to have covered the material ahead of the tutorials. There are two weekly tutorial classes delivered by the following course GTAs, starting in week 3. The schedule will be posted on MyGlasgow . Analytical Tutorials The classes are arranged to practice analytical problem sets. The first two weeks provide a brief summary of matrix calculus and statistical inference: Hadi Movaghari (i) Mondays 9-10, (ii) Thursdays 4-5, (iii) Fridays 5-6 TA Office Hours Software Tutorials The classes are arranged to build up computational foundations to work with data and methodological frameworks: Tongtong Wang (i) Mondays 4-5, (ii) Mondays 5-6, (iii) Thursdays 5-6 TA Office Hours Fridays 1-2pm Computations Financial Datasets and Empirical Exercises The course contents, practice problem sets and assessment components are based on real-world financial data. It is a requirement that all class participants set up their accounts with the data platforms described below: Register your accounts on Financial Analysis Made Easy (FAME) via the university library and additionally Wharton Research Data Services directly on their platform using the university email address. This registration is then activated by the business database administration within one week. Please initiate the registration in the first week of the course before we progress towards further course contents and assignments. Key statistics and learning outcomes arising from the activities related to the data will be part of the exam. Treat the empirical exercises as an essential part of the learning experience As a financial analyst or a research financial economist, you will work with the very same data providers repeatedly. Developing an understanding of the empirical counterparts of theories will be an important takeaway for future careers in finance. Software Packages and Implementations Computational and methodological frameworks are implemented in Matlab . An additional spreadsheet is needed for supplementary data transformation and visual inspection, e.g. Libre Office, AWK or Excel (with Analysis ToolPak and Solver Add-in packages enabled). Please make sure you have set up both packages during the first week of the course to be able to practice exercises, replicate examples and complete assignments. Computational Requirements All course material and exercises are designed such that the learning outcomes are achieved based on any computer. However, you may also prefer to consider exploring the following available options to enhance computational capacity and further familiarising yourselves with professional computing systems: University HPC Access to HPC machines are provided for research and education purposes. You will be able to access these resources depending on your computational requirements. Google Cloud : Machine template is KX8765D, you will need to set up a new machine following the template ID which provides limited free service for the purpose of the class exercises. Problem Sets Problem Set 1 Problem Set 2 Assessments The course summative assessment comprises the following four components: Quiz (15%) will be made available to access on during week 4 via Moodle. The quiz will be accessible to start within a 24 hours window, and once started the allowed time to complete is 60 minutes. This is an individual assessment and only one attempt is allowed. The quiz comprises multiple-choice questions covering course contents during the first four weeks including methodological learning outcomes, key facts and statistics arising from the numerical and empirical exercises. Group Assignment (25%) includes a problem sheet requiring methodological derivations, numerical computations followed by interpretation of results. The problem sheet will be posted during mid February.. Degree exam in April/May (60%): The final exam will be an individual assessment covering all course contents during the semester including key facts and statistics arising from empirical exercises, class reports and commentaries, methodological derivations and computations. Information regarding the final examination will be released towards the end of the semester. Grading Grading is based on meeting the course intended learning outcomes examined in each assignment and following the University's Schedule A . Grades are rewarded based on both the input and output presented in each part thus demonstrating intermediate steps building up towards an overall answer are required and graded. Problem set and assignments require accessing real-world financial data from the professional platforms, thus class participants are required to register and activate their accounts with data providers by following the information provided. Feedback Answers to the assignments will be provided in the subsequent week after the deadline and after everyone's submissions are received. Aside from the assessed assignments indicated above, the course includes two practice problem sets with solutions. These are distributed to practice theories and implementations during the semester. Students are expected to attend the office hours and tutorial workshops for reviewing specific queries. Past Papers Past exam papers are available via the university portal . These can serve as a basis for preparation, however, note that the exam and course contents are subject to changes on an annual basis. Textbook and Reading List Applied Data Science: Lessons Learned for the Data-Driven Business, By Braschler, Stadelmann, Stockinger, Online version available via the university library The elements of statistical learning: data mining, inference, and prediction, By Hastie, Tibshirani, Robert, Online version available via the university library Machine Learning in Business: An Introduction to the World of Data Science, by John Hull Software References: Software Handout MATLAB: a practical introduction to programming and problem solving, by Stormy Attaway, Online version available via the university library Further to the textbooks, there will be journal article readings cited throughout the course. Journal articles indicated as 'required reading' should also be studied in conjunction with textbook reading and form part of the assessments: Reading List .","title":"Data Science and Machine Learning in Finance"},{"location":"pages/Teaching/accfin5246/#overview","text":"Addressing real-world economic and financial problems via information embedded in data is an active area of academic and professional interests. This course contributes towards this goal with the following two approaches. First, the course provides a foundation to methodically structure large-dimensional datasets and summarise information into interpretable outcomes. The second part of the course examines how the combination of large datasets accompanied by statistical learning and artificial intelligence techniques are helping practitioners to make more efficient economic and financial decisions. The cwourse is delivered based on a balanced combination of (1) descriptive contents required to formulate financial and economic problems into quantifiable objects of interest, (2) analytical derivations and statistical techniques and (3) software programming. The course is structured based on four pillars. All pillars are equally weighted in terms of course contents and examinations but also in terms of their importance towards developing a foundation for future careers in data science and machine learning in finance within or outside academia: Data Methodological Frameworks Software Implementations Finance Theory and Applications","title":"Overview"},{"location":"pages/Teaching/accfin5246/#course-contents","text":"Data Science: Theories and Implementations Regression Analysis: Theory Regression Analysis: Implementations and Applications Machine Learning: Theory Machine Learning: Implementations and Applications","title":"Course Contents"},{"location":"pages/Teaching/accfin5246/#prerequisites","text":"There is no formal prerequisite, however a prior background including calculus, statistics and regression is favourable. The course assumes familiarity with the estimation and inference of the least squares framework covered earlier in the semester one courses.","title":"Prerequisites"},{"location":"pages/Teaching/accfin5246/#course-timetable","text":"The course is delivered via weekly sessions and four tutorial workshops. There are three practice problem sets with solutions to further illustrate theories and implementations, followed by three assessment assignments outlined in the semester timetable below. The timetable below is subject to change, please review this timetable on weekly basis: Course Schedule","title":"Course Timetable"},{"location":"pages/Teaching/accfin5246/#office-hours","text":"Friday 9-10 am (Gilbert Scott Building)","title":"Office Hours"},{"location":"pages/Teaching/accfin5246/#course-tutorials-and-gta-support","text":"You are expected to have covered the material ahead of the tutorials. There are two weekly tutorial classes delivered by the following course GTAs, starting in week 3. The schedule will be posted on MyGlasgow .","title":"Course Tutorials and GTA Support"},{"location":"pages/Teaching/accfin5246/#analytical-tutorials","text":"The classes are arranged to practice analytical problem sets. The first two weeks provide a brief summary of matrix calculus and statistical inference: Hadi Movaghari (i) Mondays 9-10, (ii) Thursdays 4-5, (iii) Fridays 5-6 TA Office Hours","title":"Analytical Tutorials"},{"location":"pages/Teaching/accfin5246/#software-tutorials","text":"The classes are arranged to build up computational foundations to work with data and methodological frameworks: Tongtong Wang (i) Mondays 4-5, (ii) Mondays 5-6, (iii) Thursdays 5-6 TA Office Hours Fridays 1-2pm","title":"Software Tutorials"},{"location":"pages/Teaching/accfin5246/#computations","text":"","title":"Computations"},{"location":"pages/Teaching/accfin5246/#financial-datasets-and-empirical-exercises","text":"The course contents, practice problem sets and assessment components are based on real-world financial data. It is a requirement that all class participants set up their accounts with the data platforms described below: Register your accounts on Financial Analysis Made Easy (FAME) via the university library and additionally Wharton Research Data Services directly on their platform using the university email address. This registration is then activated by the business database administration within one week. Please initiate the registration in the first week of the course before we progress towards further course contents and assignments. Key statistics and learning outcomes arising from the activities related to the data will be part of the exam. Treat the empirical exercises as an essential part of the learning experience As a financial analyst or a research financial economist, you will work with the very same data providers repeatedly. Developing an understanding of the empirical counterparts of theories will be an important takeaway for future careers in finance.","title":"Financial Datasets and Empirical Exercises"},{"location":"pages/Teaching/accfin5246/#software-packages-and-implementations","text":"Computational and methodological frameworks are implemented in Matlab . An additional spreadsheet is needed for supplementary data transformation and visual inspection, e.g. Libre Office, AWK or Excel (with Analysis ToolPak and Solver Add-in packages enabled). Please make sure you have set up both packages during the first week of the course to be able to practice exercises, replicate examples and complete assignments.","title":"Software Packages and Implementations"},{"location":"pages/Teaching/accfin5246/#computational-requirements","text":"All course material and exercises are designed such that the learning outcomes are achieved based on any computer. However, you may also prefer to consider exploring the following available options to enhance computational capacity and further familiarising yourselves with professional computing systems: University HPC Access to HPC machines are provided for research and education purposes. You will be able to access these resources depending on your computational requirements. Google Cloud : Machine template is KX8765D, you will need to set up a new machine following the template ID which provides limited free service for the purpose of the class exercises.","title":"Computational Requirements"},{"location":"pages/Teaching/accfin5246/#problem-sets","text":"Problem Set 1 Problem Set 2","title":"Problem Sets"},{"location":"pages/Teaching/accfin5246/#assessments","text":"The course summative assessment comprises the following four components: Quiz (15%) will be made available to access on during week 4 via Moodle. The quiz will be accessible to start within a 24 hours window, and once started the allowed time to complete is 60 minutes. This is an individual assessment and only one attempt is allowed. The quiz comprises multiple-choice questions covering course contents during the first four weeks including methodological learning outcomes, key facts and statistics arising from the numerical and empirical exercises. Group Assignment (25%) includes a problem sheet requiring methodological derivations, numerical computations followed by interpretation of results. The problem sheet will be posted during mid February.. Degree exam in April/May (60%): The final exam will be an individual assessment covering all course contents during the semester including key facts and statistics arising from empirical exercises, class reports and commentaries, methodological derivations and computations. Information regarding the final examination will be released towards the end of the semester.","title":"Assessments"},{"location":"pages/Teaching/accfin5246/#grading","text":"Grading is based on meeting the course intended learning outcomes examined in each assignment and following the University's Schedule A . Grades are rewarded based on both the input and output presented in each part thus demonstrating intermediate steps building up towards an overall answer are required and graded. Problem set and assignments require accessing real-world financial data from the professional platforms, thus class participants are required to register and activate their accounts with data providers by following the information provided.","title":"Grading"},{"location":"pages/Teaching/accfin5246/#feedback","text":"Answers to the assignments will be provided in the subsequent week after the deadline and after everyone's submissions are received. Aside from the assessed assignments indicated above, the course includes two practice problem sets with solutions. These are distributed to practice theories and implementations during the semester. Students are expected to attend the office hours and tutorial workshops for reviewing specific queries.","title":"Feedback"},{"location":"pages/Teaching/accfin5246/#past-papers","text":"Past exam papers are available via the university portal . These can serve as a basis for preparation, however, note that the exam and course contents are subject to changes on an annual basis.","title":"Past Papers"},{"location":"pages/Teaching/accfin5246/#textbook-and-reading-list","text":"Applied Data Science: Lessons Learned for the Data-Driven Business, By Braschler, Stadelmann, Stockinger, Online version available via the university library The elements of statistical learning: data mining, inference, and prediction, By Hastie, Tibshirani, Robert, Online version available via the university library Machine Learning in Business: An Introduction to the World of Data Science, by John Hull Software References: Software Handout MATLAB: a practical introduction to programming and problem solving, by Stormy Attaway, Online version available via the university library Further to the textbooks, there will be journal article readings cited throughout the course. Journal articles indicated as 'required reading' should also be studied in conjunction with textbook reading and form part of the assessments: Reading List .","title":"Textbook and Reading List"},{"location":"pages/Teaching/dissertations/","text":"I welcome ACCFIN4001 research proposals within the finance and financial economics pathways, more specifically: Macro-Finance Financial Valuation Banking and Financial Intermediation Theoretical founcations, empirical setting and databases. You are expected to adhere to ACCFIN4001 course guidelines as characterised by the main dissertation convenor. The following descriptions are applicable only if you choose to develop your dissertation under my supervision . Research Aim The aim needs to be within the finance or financial economics pathways. The aim ideally should tackle an interesting whilst feasible topic. The overall objective naturally sits within your interests but also ideally aligned with your future career or graduate studies. Literature Review Please consider using a reference management software such as EndNote , Mendeley Software Depending on your research, it's very likely that you will need to rely on at least one analytical or computational software. Most empirical studies carried out under ACCFIN4001 can be completed in Excel Excel with Data Analytics Toolbox Stata Mathematica Matlab You can use any software of choice to carry out empirical or analytical computations. However, please ensure your research objectives are inline with your prior software skills. You will be able to develop your software skills particularly if you have prior acquaintance. If you have no or very limited prior experience, please note this in your research proposal so that we can gauge whether the completions can be achieved considering additional training. Higher level computing requirements can be carried out via the university HPC services University HPC Information Access to HPC machines are provided for research purposes. You will be able to access these resources depending on your dissertation's computational requirements. Reading List Dissertation Notes The Little Book of Research Writing Previous Dissertation Topics An Investigation into the Emergence of Fintech and how it has Impacted the UK Banking Industry, as Measured by a Few Specific Indicators Deal or No Deal? Investigating the Effects of the 2008 Financial Crisis on UK M&A Deal Activity Do Female Directors Affect Firm Performance in the UK: A FTSE 350 Investigation","title":"Dissertations"},{"location":"pages/Teaching/dissertations/#research-aim","text":"The aim needs to be within the finance or financial economics pathways. The aim ideally should tackle an interesting whilst feasible topic. The overall objective naturally sits within your interests but also ideally aligned with your future career or graduate studies.","title":"Research Aim"},{"location":"pages/Teaching/dissertations/#literature-review","text":"Please consider using a reference management software such as EndNote , Mendeley","title":"Literature Review"},{"location":"pages/Teaching/dissertations/#software","text":"Depending on your research, it's very likely that you will need to rely on at least one analytical or computational software. Most empirical studies carried out under ACCFIN4001 can be completed in Excel Excel with Data Analytics Toolbox Stata Mathematica Matlab You can use any software of choice to carry out empirical or analytical computations. However, please ensure your research objectives are inline with your prior software skills. You will be able to develop your software skills particularly if you have prior acquaintance. If you have no or very limited prior experience, please note this in your research proposal so that we can gauge whether the completions can be achieved considering additional training. Higher level computing requirements can be carried out via the university HPC services","title":"Software"},{"location":"pages/Teaching/dissertations/#university-hpc-information","text":"Access to HPC machines are provided for research purposes. You will be able to access these resources depending on your dissertation's computational requirements.","title":"University HPC Information"},{"location":"pages/Teaching/dissertations/#reading-list","text":"Dissertation Notes The Little Book of Research Writing","title":"Reading List"},{"location":"pages/Teaching/dissertations/#previous-dissertation-topics","text":"An Investigation into the Emergence of Fintech and how it has Impacted the UK Banking Industry, as Measured by a Few Specific Indicators Deal or No Deal? Investigating the Effects of the 2008 Financial Crisis on UK M&A Deal Activity Do Female Directors Affect Firm Performance in the UK: A FTSE 350 Investigation","title":"Previous Dissertation Topics"},{"location":"pages/Teaching/cfr/","text":"The goals of this course is to demonstrates the applications of software computations to Financial and economic decisions are determined by wide-ranging drivers. Human behaviour, preferences for consumption and investment products, supply-side fundamentals, policy and regulatory responses and many other factors determine the world of finance and economics. Such drivers are often times explicitly recorded in data or indirectly captured by alternative means that are transformable to quantitative or numeric metrics. The availability and growth of data requires appropriate computational platforms to process and organise information in its unstructured form to actionable and interpretable forms. Course Outline Basics Data Processing Statistical Analysis Numerical and Computational Tools Applications Prerequisites There is no formal prerequisite, however a prior background including calculus, statistics and regression is favourable. The course assumes familiarity with the estimation and inference of the least squares framework covered earlier in the semester one courses. Datasets Financial Analysis Health Economics Code Files Practice Code 1 Practice Code 2 Reading The course primarily relies on the first reference below: Garrett Wickham, Garrett Grolemund , 2017, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data , ISBN-13: 978-1491910399 John Braun , 2021, A First Course in Statistical Programming with R , ISBN-13: 978-1108995146 Christensen, T.M., Hurn, A.S. and Lindsay, K.A. , 2008. The devil is in the detail: hints for practical optimisation . Economic Analysis and Policy, 38(2), pp.345-368. Ken Kelley, Keke Lai and Po-Ju Wu , 2008,Best Practices in Quantiative Methods, Chapter 34 Using R for Data Analysis: A Best Practice for Research , Further research-level readings are provided by The R Journal presenting peer-reviewed and open-access material complied on the computational operations and applications of the software to specialised purposes.","title":"Overview"},{"location":"pages/Teaching/cfr/#course-outline","text":"Basics Data Processing Statistical Analysis Numerical and Computational Tools Applications","title":"Course Outline"},{"location":"pages/Teaching/cfr/#prerequisites","text":"There is no formal prerequisite, however a prior background including calculus, statistics and regression is favourable. The course assumes familiarity with the estimation and inference of the least squares framework covered earlier in the semester one courses.","title":"Prerequisites"},{"location":"pages/Teaching/cfr/#datasets","text":"Financial Analysis Health Economics","title":"Datasets"},{"location":"pages/Teaching/cfr/#code-files","text":"Practice Code 1 Practice Code 2","title":"Code Files"},{"location":"pages/Teaching/cfr/#reading","text":"The course primarily relies on the first reference below: Garrett Wickham, Garrett Grolemund , 2017, R for Data Science: Import, Tidy, Transform, Visualize, and Model Data , ISBN-13: 978-1491910399 John Braun , 2021, A First Course in Statistical Programming with R , ISBN-13: 978-1108995146 Christensen, T.M., Hurn, A.S. and Lindsay, K.A. , 2008. The devil is in the detail: hints for practical optimisation . Economic Analysis and Policy, 38(2), pp.345-368. Ken Kelley, Keke Lai and Po-Ju Wu , 2008,Best Practices in Quantiative Methods, Chapter 34 Using R for Data Analysis: A Best Practice for Research , Further research-level readings are provided by The R Journal presenting peer-reviewed and open-access material complied on the computational operations and applications of the software to specialised purposes.","title":"Reading"},{"location":"pages/Teaching/cfr/p10/","text":"","title":"P10"},{"location":"pages/Teaching/cfr/p11/","text":"hide: - title - footer","title":"P11"},{"location":"pages/Teaching/cfr/p12/","text":"hide: - title - footer","title":"P12"},{"location":"pages/Teaching/cfr/p13/","text":"hide: - title - footer","title":"P13"},{"location":"pages/Teaching/cfr/p2/","text":"Importing and Exporting Data","title":"P2"},{"location":"pages/Teaching/cfr/p2/#importing-and-exporting-data","text":"","title":"Importing and Exporting Data"},{"location":"pages/Teaching/cfr/p3/","text":"Visualization","title":"P3"},{"location":"pages/Teaching/cfr/p3/#visualization","text":"","title":"Visualization"},{"location":"pages/Teaching/cfr/p7/","text":"The higher-level features of the R program enables interactions with external resources. More specifically, R program conducts information exchange with external databases to directly collect and load datasets into the local memory to bypass separate data collection. The information exchange features also offers export to remote or cloud spaces and is considered a standard productivity tool for collaborative workflows. The following features illustrate each feature based on their properties and suitability depending on their application contexts. Fetch Web Write Github Open Database Connectivity (ODBC)","title":"P7"},{"location":"pages/Teaching/cfr/p7/#fetch","text":"","title":"Fetch"},{"location":"pages/Teaching/cfr/p7/#web-write","text":"","title":"Web Write"},{"location":"pages/Teaching/cfr/p7/#github","text":"","title":"Github"},{"location":"pages/Teaching/cfr/p7/#open-database-connectivity-odbc","text":"","title":"Open Database Connectivity (ODBC)"},{"location":"pages/Teaching/cfr/p8/","text":"hpc","title":"P8"},{"location":"pages/Teaching/cfr/rp1/","text":"Applied Statistical Computing (R) The history of computational techniques in informing business decisions commonly known as business analytics began in the late 60s with the rise of rudimentary computers enabling their users to process information faster and more reliably. In particular, these platforms lowered the cost of storage and retrieval of information by using databases from early days, enabled repeated retrial of hypothetical scenarios to simulate numerous possibilities to acquire a quantitative basis for projecting the worse and best case scenarios. An early example from 1960 In the 1960s only the government and large enterprises could afford computers nonetheless the cost was considered justified comparing to the equivalent processing output per unit of information. The computational technology was considered scalable from early days providing expandable features to grow exceedingly fast further justifying the costs. Modern financial and business decisions are inevitably dependent on 100s of moving components measured along fast moving settings, thus it is quintessential to equip businesses with the appropriate computational requirements, notably software routines or within the context of this course software code , to efficiently acquire, process and output information. R Foundation The R Foundation for Statistical Computing created the software with notable statisticians Ross Ihaka and Robert Gentleman with the first versions appearing in 1993. The software is named after its core creators. Each software, e.g. R, Python, Matlab, Stata, SAS, etc. offers certain similarities and unique features relative to the alternatives. The choice of the appropriate software platform is often context-dependent and is determined by the software's cost, available resources, processing requirements and processing speed, integration into an existing business universe, etc. The R software is a freeware and universally considered as one of the top packages in the statistics academic and professional communities. Each software offers a certain degree of flexibility to its users to design bespoke routines based on techniques and resources while also offering pre-defined tools (often meaning lower flexibility) in the interest of obtaining results without re-developing several intermediate phases. For instance, SPSS as an alternative package often used within similar contexts offers tools to obtain results based on fewer steps, while also limiting the users to a set of particular built-in options which may be considered suitable only for certain scenarios. Spreadsheet platforms are used across several business settings offering quick means to input, process and output information. While these platforms are considered suitable solutions to examine small datasets e.g. <5GB, processing larger scenarios become exceedingly slow and bounded by limited available statistical programming functions. Higher level software packages such as R provide a suitable environment to efficiently load and preserve datasets, develop the software code and apply intended computations and store outcomes into separate spaces. This standardised working routine and use of computer resources enhances efficiently, flexibility and reliability of the overall data handling process and obtaining results with minimal resources (both human inputs and hardware requirements). During the past decade, statistics show that 18% of businesses primarily using spreadsheets have suffered data losses while 42% and 49% of businesses also reported analytics-based underperformance and excessively costly information processing, respectively, due to the use of spreadsheets for processing large datasets. This undesirable outcome translate to financial losses either directly due to inefficient decision making, loss of clients or operating costs. TIOBE Software Popularity Index The R Programming Language has been ranked between numbers 8 and 73 by the TIOBE Index during 2007-2023 with the current standing at 19th position. The ranking captures an overall popularity amongst a wide range of object oriented, database and computational languages. Note while all packages share capabilities to carry out numerical and dataset computations, several of the higher rating languages are often considered less focused on statistical analysis. Integrating the consensus within our contexts together with the TIOBE index ranks R after Python, Matlab, and Fortran. R is considered a medium-level general purpose software language offering both flexibility and productivity based on available resources. While the software is used across several contexts, it's core uses are within the data exploration, visualization, and statistical analysis thus the existing libraries and functions are further developed around such scopes. Learning the language is not exempt from the process of learning a language in general, which requires memorising the vocabulary and establishing grammatical mechanics and learning by practice.","title":"Background"},{"location":"pages/Teaching/cfr/rp1/#applied-statistical-computing-r","text":"The history of computational techniques in informing business decisions commonly known as business analytics began in the late 60s with the rise of rudimentary computers enabling their users to process information faster and more reliably. In particular, these platforms lowered the cost of storage and retrieval of information by using databases from early days, enabled repeated retrial of hypothetical scenarios to simulate numerous possibilities to acquire a quantitative basis for projecting the worse and best case scenarios. An early example from 1960 In the 1960s only the government and large enterprises could afford computers nonetheless the cost was considered justified comparing to the equivalent processing output per unit of information. The computational technology was considered scalable from early days providing expandable features to grow exceedingly fast further justifying the costs. Modern financial and business decisions are inevitably dependent on 100s of moving components measured along fast moving settings, thus it is quintessential to equip businesses with the appropriate computational requirements, notably software routines or within the context of this course software code , to efficiently acquire, process and output information.","title":"Applied Statistical Computing (R)"},{"location":"pages/Teaching/cfr/rp1/#r-foundation","text":"The R Foundation for Statistical Computing created the software with notable statisticians Ross Ihaka and Robert Gentleman with the first versions appearing in 1993. The software is named after its core creators. Each software, e.g. R, Python, Matlab, Stata, SAS, etc. offers certain similarities and unique features relative to the alternatives. The choice of the appropriate software platform is often context-dependent and is determined by the software's cost, available resources, processing requirements and processing speed, integration into an existing business universe, etc. The R software is a freeware and universally considered as one of the top packages in the statistics academic and professional communities. Each software offers a certain degree of flexibility to its users to design bespoke routines based on techniques and resources while also offering pre-defined tools (often meaning lower flexibility) in the interest of obtaining results without re-developing several intermediate phases. For instance, SPSS as an alternative package often used within similar contexts offers tools to obtain results based on fewer steps, while also limiting the users to a set of particular built-in options which may be considered suitable only for certain scenarios. Spreadsheet platforms are used across several business settings offering quick means to input, process and output information. While these platforms are considered suitable solutions to examine small datasets e.g. <5GB, processing larger scenarios become exceedingly slow and bounded by limited available statistical programming functions. Higher level software packages such as R provide a suitable environment to efficiently load and preserve datasets, develop the software code and apply intended computations and store outcomes into separate spaces. This standardised working routine and use of computer resources enhances efficiently, flexibility and reliability of the overall data handling process and obtaining results with minimal resources (both human inputs and hardware requirements). During the past decade, statistics show that 18% of businesses primarily using spreadsheets have suffered data losses while 42% and 49% of businesses also reported analytics-based underperformance and excessively costly information processing, respectively, due to the use of spreadsheets for processing large datasets. This undesirable outcome translate to financial losses either directly due to inefficient decision making, loss of clients or operating costs. TIOBE Software Popularity Index The R Programming Language has been ranked between numbers 8 and 73 by the TIOBE Index during 2007-2023 with the current standing at 19th position. The ranking captures an overall popularity amongst a wide range of object oriented, database and computational languages. Note while all packages share capabilities to carry out numerical and dataset computations, several of the higher rating languages are often considered less focused on statistical analysis. Integrating the consensus within our contexts together with the TIOBE index ranks R after Python, Matlab, and Fortran. R is considered a medium-level general purpose software language offering both flexibility and productivity based on available resources. While the software is used across several contexts, it's core uses are within the data exploration, visualization, and statistical analysis thus the existing libraries and functions are further developed around such scopes. Learning the language is not exempt from the process of learning a language in general, which requires memorising the vocabulary and establishing grammatical mechanics and learning by practice.","title":"R Foundation"},{"location":"pages/Teaching/cfr/rp10/","text":"Alternative Data Geo-Data Images Textual Analysis Natural Language Processing Audio","title":"Rp10"},{"location":"pages/Teaching/cfr/rp10/#alternative-data","text":"","title":"Alternative Data"},{"location":"pages/Teaching/cfr/rp10/#geo-data","text":"","title":"Geo-Data"},{"location":"pages/Teaching/cfr/rp10/#images","text":"","title":"Images"},{"location":"pages/Teaching/cfr/rp10/#textual-analysis","text":"","title":"Textual Analysis"},{"location":"pages/Teaching/cfr/rp10/#natural-language-processing","text":"","title":"Natural Language Processing"},{"location":"pages/Teaching/cfr/rp10/#audio","text":"","title":"Audio"},{"location":"pages/Teaching/cfr/rp2/","text":"To use the software for data exploration, visualization, and statistical analysis, two main installations are required: Software Core Engine : The software core platform is developed by the R-Project and can be downloaded from the Installation Page . While the core platform provides a basis including the main abilities, it is considered less user-friendly thus an additional user-interface is needed to utilise the software to a fuller extent. R-Studio : A graphical user-interface \" R-Studio \" provides the suitable tools to apply the core engine to data and statistical analyses, e.g. separately developing and executing code, provide a visual interface to inspect and manage existing variables, etc. This additional platform serves as an Integrated Development Environment (IDE) which acts in between the basis core engine and the user and enables accessing and managing resources. R-Studio is not the only user-interface and alternatives can be used. Cloud-Based Environments are also available to enable web-based computations. While both the core engine (1) and R-Studio are freeware packages, other desktop- or cloud-based solutions may be subject to fees. After installations, the software environment is ready for launch. As a third step, additional libraries are required to be installed. Each library provides specialised toolboxes intended for further productivity such as visualisation, obtaining statistical properties and similar context-dependent features (explained in the page sections). R-Studio R Engine R Engine and R-Studio The R Programming Language has been ranked between numbers 8 and 73 by the TIOBE Index during 2007-2023 with the current standing at 19th position. The ranking captures an overall popularity amongst a wide range of object-oriented, database and computational languages. Note while all packages share capabilities to carry out numerical and dataset computations, several of the higher rating languages are often considered less focused on statistical analysis. Integrating the consensus within our contexts together with the TIOBE index ranks R after Python, Matlab, and Fortran. R Engine and R-Studio The R Programming Language has been ranked between numbers 8 and 73 by the TIOBE Index during 2007-2023 with the current standing at 19th position. The ranking captures an overall popularity amongst a wide range of object oriented, database and computational languages. Note while all packages share capabilities to carry out numerical and dataset computations, several of the higher rating languages are often considered less focused on statistical analysis. Integrating the consensus within our contexts together with the TIOBE index ranks R after Python, Matlab, and Fortran. The course refers to the R-Studio henceforth as the primary interface. R-Studio provide four main panels: Top left (as illustrated above) is the Editor where operation are coded line by line without executing. This panel is used to developing sections of code and editing code in preparation for executing. Each line is numbered and later executed in the same order. Top right panel is the workspace providing a visual illustration of the existing objects already created and stored in the software. These objects (variables, numbers, vectors, etc.) are accessible by the developer and can be called into operations, deleted or replaced. Bottom left panel is the RConsole where a user can interact with the software via entering separate lines of code which are instantly executed. Bottom right panel provides additional interfaces such as the working directory where the software assumes as the default filing location to input and output information. It is essential to match the location of the working directory correctly to the intended location where data files are stored otherwise the software is unable to access and operate on the data. Interacting with R-Studio The editor window is a coding space to develop the code and prepare for execution. The software then executes each line by applying the functions or commands to the software space, variables, etc. note that the editor window disregards code or lines of code when '#' is entered which instructs the software to skip over the contents after '#' within the same line. Commenting : Using '#' is considered a good practice to add comments to the same coding environment for the purpose of informing readers (or for future references). This commonly is considered a requirement when working as a part of a teamwork where the code is partially developed and passed to other contributors for further implementations where comments provide further context for others to better follow the logic behind the coded sections, variable naming practices, external references, etc. Executing Code : Pressing ctrl+enter runs the entire code contents in the editor window. Highlighting a subsection of the code followed by pressing ctrl + enter amounts to the selection of the code to be executed. As a counterpart, once a section of the code is selected for execution, the operation can be cancelled by repeating ctrl+enter. Note that for short sections of code, the execution takes place very fast and there is limited scope to attempt cancellation. Nevertheless, longer segments of code with several iterations may take longer to run and cancellation is important to abort and amend. Info It is essential to ensure the working directory is correctly set up. This involves instructing R to assume a default filing position on the computer via setwd(\"location\") command displayed below, where the location indicates a local address. Once a working directory is set up, it subsequently can be checked via getwd() where the parentheses is left blank. Executing getwd() leads to the original location address to be printed out on the output window: Code Results (Console) Setting Working Directory 1 2 setwd(\"C:/r/class1\") # Set up the working directory getwd() # print out assumed location Interaction with the software leads to storing new information in the memory or changing the default setting. When starting a new session in R-Studio, the software begins with a clean workspace. However, after interacting with the software through only a few commands, the memory tracks inputted commands and changes according to the interactions. Suppose instead of directly entering the directory address into the setwd(\"C:/r/class1\") , the code is set up to first store the directory address as a text value into a variable x = \"C:/r/class1\" which then carries its value to the next line for setting the address as the default working directory: Code Results (Workspace Variable Created) Results (Workspace Variable Cleared) Clearing Memory 1 2 3 4 5 x = \"C:/r/class1\" # New text variable setwd(x) # Set up the working directory getwd() # print out assumed location rm(list=ls()) # Clear working space (variables, memory) cat(\"\\f\") # Clear console Clearing Memory 1 2 3 4 5 x = \"C:/r/class1\" # New text variable setwd(x) # Set up the working directory getwd() # print out assumed location rm(list=ls()) # Clear working space (variables, memory) cat(\"\\f\") # Clear console Clearing Memory 1 2 3 4 5 x = \"C:/r/class1\" # New text variable setwd(x) # Set up the working directory getwd() # print out assumed location rm(list=ls()) # Clear working space (variables, memory) cat(\"\\f\") # Clear console In the example above the value x is stored in the software workspace and remains as an accessible information for future use. When needed, this added value can be erased via the command in line 4: rm(list=ls()) . Note that the entire workspace is cleared including the value for x . Deleting individual variables is possible via ... The coded section above can be copied into the editor window and saved as a .Rmd file as the default saving format. Basic Objects Operators Commands Variables Functions Accessing Built-in Help Documentations Code Results (Bottom-right Panel) Accessing Help Files 1 2 ?setwd() ?getwd()","title":"User Interface"},{"location":"pages/Teaching/cfr/rp2/#interacting-with-r-studio","text":"The editor window is a coding space to develop the code and prepare for execution. The software then executes each line by applying the functions or commands to the software space, variables, etc. note that the editor window disregards code or lines of code when '#' is entered which instructs the software to skip over the contents after '#' within the same line. Commenting : Using '#' is considered a good practice to add comments to the same coding environment for the purpose of informing readers (or for future references). This commonly is considered a requirement when working as a part of a teamwork where the code is partially developed and passed to other contributors for further implementations where comments provide further context for others to better follow the logic behind the coded sections, variable naming practices, external references, etc. Executing Code : Pressing ctrl+enter runs the entire code contents in the editor window. Highlighting a subsection of the code followed by pressing ctrl + enter amounts to the selection of the code to be executed. As a counterpart, once a section of the code is selected for execution, the operation can be cancelled by repeating ctrl+enter. Note that for short sections of code, the execution takes place very fast and there is limited scope to attempt cancellation. Nevertheless, longer segments of code with several iterations may take longer to run and cancellation is important to abort and amend. Info It is essential to ensure the working directory is correctly set up. This involves instructing R to assume a default filing position on the computer via setwd(\"location\") command displayed below, where the location indicates a local address. Once a working directory is set up, it subsequently can be checked via getwd() where the parentheses is left blank. Executing getwd() leads to the original location address to be printed out on the output window: Code Results (Console) Setting Working Directory 1 2 setwd(\"C:/r/class1\") # Set up the working directory getwd() # print out assumed location Interaction with the software leads to storing new information in the memory or changing the default setting. When starting a new session in R-Studio, the software begins with a clean workspace. However, after interacting with the software through only a few commands, the memory tracks inputted commands and changes according to the interactions. Suppose instead of directly entering the directory address into the setwd(\"C:/r/class1\") , the code is set up to first store the directory address as a text value into a variable x = \"C:/r/class1\" which then carries its value to the next line for setting the address as the default working directory: Code Results (Workspace Variable Created) Results (Workspace Variable Cleared) Clearing Memory 1 2 3 4 5 x = \"C:/r/class1\" # New text variable setwd(x) # Set up the working directory getwd() # print out assumed location rm(list=ls()) # Clear working space (variables, memory) cat(\"\\f\") # Clear console Clearing Memory 1 2 3 4 5 x = \"C:/r/class1\" # New text variable setwd(x) # Set up the working directory getwd() # print out assumed location rm(list=ls()) # Clear working space (variables, memory) cat(\"\\f\") # Clear console Clearing Memory 1 2 3 4 5 x = \"C:/r/class1\" # New text variable setwd(x) # Set up the working directory getwd() # print out assumed location rm(list=ls()) # Clear working space (variables, memory) cat(\"\\f\") # Clear console In the example above the value x is stored in the software workspace and remains as an accessible information for future use. When needed, this added value can be erased via the command in line 4: rm(list=ls()) . Note that the entire workspace is cleared including the value for x . Deleting individual variables is possible via ... The coded section above can be copied into the editor window and saved as a .Rmd file as the default saving format.","title":"Interacting with R-Studio"},{"location":"pages/Teaching/cfr/rp2/#basic-objects","text":"Operators Commands Variables Functions","title":"Basic Objects"},{"location":"pages/Teaching/cfr/rp2/#accessing-built-in-help-documentations","text":"Code Results (Bottom-right Panel) Accessing Help Files 1 2 ?setwd() ?getwd()","title":"Accessing Built-in Help Documentations"},{"location":"pages/Teaching/cfr/rp3/","text":"Information is stored within variable objects. A variable object is akin to a shelving concept described by a name - which starts with lower or upper case letter. R program is unable to create variable names starting with numbers or special characters e.g. _ . The name may involve numbers or _ following the first letter e.g. x1 or x_1 . an encasement to hold data, and contents inside the encasement. Naming System It is essential to set up variable names according to a methodical or systematic naming practice. For instance, variables associated with a particular data type e.g. balance sheet or health insurance, etc. to start with a common letter and more desirably compact representations such as hins_partA_01 , hins_partB_01 , etc. This enables more efficient workspace management as well as better collaborative workflow optimization. The three aspects above fully characterise a variable object that is ready to be called or used for computations. Assignment Operators = & <- Variables are populated based on software user inputted values or imported data. The former is carried out according to two approaches: Using '=' such as x = 1 instructs the software store the right-hand-side value into the left-hand-side space. Within the context of R language, '=' is considered a weaker assignment instruction or under certain condition a temporary assignment. Consider the example below where variable x is defined within an outer command mean() . in this case, the variable is defined and used for computing the average and referred to as a subexpressions but discarded immediately after without appearing in the software workspace for future use. Assign a value to a name <- (leftwards): This operator is applicable anywhere and encompasses the functionality of = in addition to ensuring the values are stored under the workspace. Assigment (=) Assigment (<-) Calling x in line 2 returns error 1 2 mean(x = 1:2) # subexpression receives values and discards afterwards x Calling x in line 2 returns values 1 2 mean(x <- 1:2) # subexpression receives values and retains afterwards x In general both assignments are considered acceptable, however, note that the developer may intentionally aim for temporary use of variables and under such circumstances, using = is considered optimal since it leads to lower memory usage. Re-using, Over-writing and Re-allocating R language permits redefining a variable contents, without prompting a notice. This is considered a convenient feature when attempting to correct the content of a preexisting variable without encountering an error or requiring additional lines of code to confirm overwriting operation. However, a developer should exercise causion when naming variables since unintended overwrites can also take place without notifying the developer. Missing Values and Special Entries NaN : 'Not a Number' arises across numeric variables when a non-numeric e.g. text, date, etc. appears along the entries. NULL : Represents an object with zero length. This reflects a case where no allocation has been made contrary to the NaN where allocation is made with empty data stored NA : The following commands enable verifying the variables versus each of the three outcomes above. Note that the result of this verification is reported under the console panel and in a form of a True or False (logical) output: Code Results (Workspace Variable Created) Clearing Memory 1 2 3 4 5 6 7 8 9 10 11 x1 <- \"NA\" x2 <- \"NULL\" x3 <- NULL x4 <- NA x5 <- NaN x6 <- Inf is.nan(A) in.na(A) is.null(A) is.infinite(A)) Clearing Memory 1 2 3 4 5 x = \"C:/r/class1\" # New text variable setwd(x) # Set up the working directory getwd() # print out assumed location rm(list=ls()) # Clear working space (variables, memory) cat(\"\\f\") # Clear console Encasement and Contents For instance, the first row from the bottom and the second column in the illustration below is a location address to the bookshelf. Given that the shelving is in place, an array exists to hold contents. The contents in the location is the data, e.g. the numeric value 3.14 in the shelving. When the contents under the shelving is missing, then that value appears as NaN, whereas when the entire bookshelf is missing, the variable name is associated with a NULL. The contents under row one across column 3 is a text entry 'Pi' which is interpreted as NaN by the software. When interacting with numerical variables such as discrete or continuous type data, it is possible to encounter two special cases amongst the variable entries: Inf : The computer scientific definition of the infinity value in R program is 1e308 i.e. \\(10^{308}\\) . Any value larger than the threshold value is stored as Inf where calculus operations associated with Inf lead to a trivial outputs mapping either to Inf (disregarding power of magnitude) or alternative NaN as a counterbalancing result. As a side note, further operations will be possible via Analytical Algebraic System where values are inputted as symbolic entries (explained later) further enabling extended computations such as computing values at limits based on the order of magnitude. Code Results (Workspace Variable Created) Operating on Infinity 1 2 3 4 5 x = Inf y = Inf^2 x + y x - y x * y Complex : While there exists several context to apply the complex numbers (e.g. working with characteristics function in statistical analysis), the applications within the current course are rare. However, a complex entry may be encountered within regular applications due to errors in computations such as when an optimization outcome for variance estimation returns a negative value due to that numerical value achieving a maximum objective while in fact there is no real-value interpretations. Such cases may later result in encountering a complex entry since the square root of the incorrectly estimated variance over the negative real line corresponds to a complex values standard error. Code Encountering Complex Values 1 2 z = 1 + 2i # create a complex number sqrt(\u22121+0i) # square root of a negative number Complex numbers are rarely used in the context of this course, however it is important to be familiar with their overall appearances as a method of troubleshooting coding routines as they may still arise due to computational reasons.","title":"Variable Types"},{"location":"pages/Teaching/cfr/rp3/#assignment-operators-","text":"Variables are populated based on software user inputted values or imported data. The former is carried out according to two approaches: Using '=' such as x = 1 instructs the software store the right-hand-side value into the left-hand-side space. Within the context of R language, '=' is considered a weaker assignment instruction or under certain condition a temporary assignment. Consider the example below where variable x is defined within an outer command mean() . in this case, the variable is defined and used for computing the average and referred to as a subexpressions but discarded immediately after without appearing in the software workspace for future use. Assign a value to a name <- (leftwards): This operator is applicable anywhere and encompasses the functionality of = in addition to ensuring the values are stored under the workspace. Assigment (=) Assigment (<-) Calling x in line 2 returns error 1 2 mean(x = 1:2) # subexpression receives values and discards afterwards x Calling x in line 2 returns values 1 2 mean(x <- 1:2) # subexpression receives values and retains afterwards x In general both assignments are considered acceptable, however, note that the developer may intentionally aim for temporary use of variables and under such circumstances, using = is considered optimal since it leads to lower memory usage. Re-using, Over-writing and Re-allocating R language permits redefining a variable contents, without prompting a notice. This is considered a convenient feature when attempting to correct the content of a preexisting variable without encountering an error or requiring additional lines of code to confirm overwriting operation. However, a developer should exercise causion when naming variables since unintended overwrites can also take place without notifying the developer.","title":"Assignment Operators = &amp; &lt;-"},{"location":"pages/Teaching/cfr/rp3/#missing-values-and-special-entries","text":"NaN : 'Not a Number' arises across numeric variables when a non-numeric e.g. text, date, etc. appears along the entries. NULL : Represents an object with zero length. This reflects a case where no allocation has been made contrary to the NaN where allocation is made with empty data stored NA : The following commands enable verifying the variables versus each of the three outcomes above. Note that the result of this verification is reported under the console panel and in a form of a True or False (logical) output: Code Results (Workspace Variable Created) Clearing Memory 1 2 3 4 5 6 7 8 9 10 11 x1 <- \"NA\" x2 <- \"NULL\" x3 <- NULL x4 <- NA x5 <- NaN x6 <- Inf is.nan(A) in.na(A) is.null(A) is.infinite(A)) Clearing Memory 1 2 3 4 5 x = \"C:/r/class1\" # New text variable setwd(x) # Set up the working directory getwd() # print out assumed location rm(list=ls()) # Clear working space (variables, memory) cat(\"\\f\") # Clear console Encasement and Contents For instance, the first row from the bottom and the second column in the illustration below is a location address to the bookshelf. Given that the shelving is in place, an array exists to hold contents. The contents in the location is the data, e.g. the numeric value 3.14 in the shelving. When the contents under the shelving is missing, then that value appears as NaN, whereas when the entire bookshelf is missing, the variable name is associated with a NULL. The contents under row one across column 3 is a text entry 'Pi' which is interpreted as NaN by the software. When interacting with numerical variables such as discrete or continuous type data, it is possible to encounter two special cases amongst the variable entries: Inf : The computer scientific definition of the infinity value in R program is 1e308 i.e. \\(10^{308}\\) . Any value larger than the threshold value is stored as Inf where calculus operations associated with Inf lead to a trivial outputs mapping either to Inf (disregarding power of magnitude) or alternative NaN as a counterbalancing result. As a side note, further operations will be possible via Analytical Algebraic System where values are inputted as symbolic entries (explained later) further enabling extended computations such as computing values at limits based on the order of magnitude. Code Results (Workspace Variable Created) Operating on Infinity 1 2 3 4 5 x = Inf y = Inf^2 x + y x - y x * y Complex : While there exists several context to apply the complex numbers (e.g. working with characteristics function in statistical analysis), the applications within the current course are rare. However, a complex entry may be encountered within regular applications due to errors in computations such as when an optimization outcome for variance estimation returns a negative value due to that numerical value achieving a maximum objective while in fact there is no real-value interpretations. Such cases may later result in encountering a complex entry since the square root of the incorrectly estimated variance over the negative real line corresponds to a complex values standard error. Code Encountering Complex Values 1 2 z = 1 + 2i # create a complex number sqrt(\u22121+0i) # square root of a negative number Complex numbers are rarely used in the context of this course, however it is important to be familiar with their overall appearances as a method of troubleshooting coding routines as they may still arise due to computational reasons.","title":"Missing Values and Special Entries"},{"location":"pages/Teaching/cfr/rp4/","text":"Operators Logical Assignment Relational Arithmetics Clearing Memory 1 2 3 & # and | # or ! # not (negation) Clearing Memory 1 2 3 4 5 <\u2212 # Left Assignment = <<\u2212 -> # Right Assignment ->> Clearing Memory 1 2 3 4 5 6 < Less than > Greater than <= Less than or equal to >= Greater than or equal to == Equal to != Not equal to Clearing Memory 1 2 3 4 5 6 7 + Addition - Subtraction * Multiplication / Division ^ Exponent %% Modulus(Remainder from division) %/% Integer Division Exercise: \\(0^0\\) Exercise Answer Clear the workspace, program \\(0^0\\) and verify your results: Code 1 2 3 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console x = 0^0 While in mathematical analysis, the expression \\(0^0\\) is sometimes left undefined, in algebra and combinatorics, one typically defines \\(0^0 = 1\\) . In computer sciences, the standard answer is \\(0^0 = 1\\) . Exercise: \\(x!\\) Exercise Answer Clear the workspace, program the factorial value \\(100!\\) : Code 1 2 3 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console factorial(100) Note that the mathematical symbol for factorial is given by \\(x!\\) whereas within the R language, ! operates as logical negation . The factorial is computed using the function factorial(100) and equal to 9.332622e+157 . Exercise: \\(\\pi\\) Exercise Answer Clear the workspace, program the mathematical constant value \\(\\pi\\) : Code 1 2 3 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console pi Note that the mathematical symbol for factorial is given by \\(x!\\) whereas within the R language, ! operates as logical negation . The factorial is computed using the function factorial(100) and equal to 9.332622e+157 . Exercise: Exponential \\(e^x\\) , and logarithmic function \\(\\log(x)\\) , \\(\\ln(x)\\) Exercise Answer Clear the workspace, program the following: mathematical constant value \\(e^1\\) , compute \\(\\log(e)\\) . Code 1 2 3 4 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console exp(1) log(exp(1)) R language provides a default logarithmic fuction log() operating as the natural logarithm. While there exists a short hand function log10(x) to carry out the base-10 as a special case, more general cases can be defined via inputting an extended argument: log(x, base = 10) . Exercise: Price of a three-year bond Exercise Answer Clear the workspace, program the following which reflect the present value of a three-year 3% annual coupon bearing bond with the face value of $100 and a 5% market discount rate: \\(\\frac{5}{1+5\\%} + \\frac{5}{(1+5\\%)^2} + \\frac{5}{(1+5\\%)^3} + \\frac{100}{(1+5\\%)^3}\\) Code 1 2 3 4 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console exp(1) 3/(1+0.05) + 3/(1+0.05)^2 + 3/(1+0.05)^3 + 100/(1+0.05)^3 R language provides a default logarithmic fuction log() operating as the natural logarithm. While there exists a short hand function log10(x) to carry out the base-10 as a special case, more general cases can be defined via inputting an extended argument: log(x, base = 10) .","title":"Operators"},{"location":"pages/Teaching/cfr/rp4/#operators","text":"Logical Assignment Relational Arithmetics Clearing Memory 1 2 3 & # and | # or ! # not (negation) Clearing Memory 1 2 3 4 5 <\u2212 # Left Assignment = <<\u2212 -> # Right Assignment ->> Clearing Memory 1 2 3 4 5 6 < Less than > Greater than <= Less than or equal to >= Greater than or equal to == Equal to != Not equal to Clearing Memory 1 2 3 4 5 6 7 + Addition - Subtraction * Multiplication / Division ^ Exponent %% Modulus(Remainder from division) %/% Integer Division Exercise: \\(0^0\\) Exercise Answer Clear the workspace, program \\(0^0\\) and verify your results: Code 1 2 3 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console x = 0^0 While in mathematical analysis, the expression \\(0^0\\) is sometimes left undefined, in algebra and combinatorics, one typically defines \\(0^0 = 1\\) . In computer sciences, the standard answer is \\(0^0 = 1\\) . Exercise: \\(x!\\) Exercise Answer Clear the workspace, program the factorial value \\(100!\\) : Code 1 2 3 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console factorial(100) Note that the mathematical symbol for factorial is given by \\(x!\\) whereas within the R language, ! operates as logical negation . The factorial is computed using the function factorial(100) and equal to 9.332622e+157 . Exercise: \\(\\pi\\) Exercise Answer Clear the workspace, program the mathematical constant value \\(\\pi\\) : Code 1 2 3 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console pi Note that the mathematical symbol for factorial is given by \\(x!\\) whereas within the R language, ! operates as logical negation . The factorial is computed using the function factorial(100) and equal to 9.332622e+157 . Exercise: Exponential \\(e^x\\) , and logarithmic function \\(\\log(x)\\) , \\(\\ln(x)\\) Exercise Answer Clear the workspace, program the following: mathematical constant value \\(e^1\\) , compute \\(\\log(e)\\) . Code 1 2 3 4 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console exp(1) log(exp(1)) R language provides a default logarithmic fuction log() operating as the natural logarithm. While there exists a short hand function log10(x) to carry out the base-10 as a special case, more general cases can be defined via inputting an extended argument: log(x, base = 10) . Exercise: Price of a three-year bond Exercise Answer Clear the workspace, program the following which reflect the present value of a three-year 3% annual coupon bearing bond with the face value of $100 and a 5% market discount rate: \\(\\frac{5}{1+5\\%} + \\frac{5}{(1+5\\%)^2} + \\frac{5}{(1+5\\%)^3} + \\frac{100}{(1+5\\%)^3}\\) Code 1 2 3 4 rm(list=ls()) # Clear working space cat(\"\\f\") # Clear console exp(1) 3/(1+0.05) + 3/(1+0.05)^2 + 3/(1+0.05)^3 + 100/(1+0.05)^3 R language provides a default logarithmic fuction log() operating as the natural logarithm. While there exists a short hand function log10(x) to carry out the base-10 as a special case, more general cases can be defined via inputting an extended argument: log(x, base = 10) .","title":"Operators"},{"location":"pages/Teaching/cfr/rp5/","text":"Libraries are pre-defined collections of operations intended for specific purposes. The libraries are customizable to several extents and created by contributors to carry out multiple programming without re-programming all steps. To view the list of available libraries in a current sessions use either of the following commands: Code Results (Console) Installing R Libraries 1 2 search() # Show the list of loaded libraries sessionInfo() # Loaded libraries in addition to R system information When the software is freshly started, the list of available libraries appear to around ten items. Installing and calling further libraries lead to the list above to be updated for further monitroing. Installation While ordinary operations can be programmed by a developer at each instance, it is often considered more time-efficient to rely on a pre-existing packages to achieve the same end results. Libraries are required to be installed via install.packages(\"name\") as illustrated in the example below, prior to calling the tools they offer. As a general practice, libraries are stacked and called at the beginning of a large section of code to ensure they are processed before executing the subsequent computations. Code Results (Console) Results (Newly installed software) Installing R Libraries 1 2 search() # Show the list of loaded libraries sessionInfo() # Loaded libraries in addition to R system information Rtools42 Once the R program reaches a line including the command library() , it automatically searches (i) the local working directory and subsequently (ii) accesses CRAN online repository to download and install them. As a practice, developers rely on the online repositories - this has to be downloaded only once for installation, however, each R program session or separate editor code requires the intended libraries to be called in order for the subsequent components to access and apply their tools. Code Installing R Libraries 1 2 library(ggplot2) # Visualization library(tidyverse) # Data manipulation, exploration and visualization Libraries Library Repositories Library Repository","title":"Libraries"},{"location":"pages/Teaching/cfr/rp5/#installation","text":"While ordinary operations can be programmed by a developer at each instance, it is often considered more time-efficient to rely on a pre-existing packages to achieve the same end results. Libraries are required to be installed via install.packages(\"name\") as illustrated in the example below, prior to calling the tools they offer. As a general practice, libraries are stacked and called at the beginning of a large section of code to ensure they are processed before executing the subsequent computations. Code Results (Console) Results (Newly installed software) Installing R Libraries 1 2 search() # Show the list of loaded libraries sessionInfo() # Loaded libraries in addition to R system information Rtools42 Once the R program reaches a line including the command library() , it automatically searches (i) the local working directory and subsequently (ii) accesses CRAN online repository to download and install them. As a practice, developers rely on the online repositories - this has to be downloaded only once for installation, however, each R program session or separate editor code requires the intended libraries to be called in order for the subsequent components to access and apply their tools. Code Installing R Libraries 1 2 library(ggplot2) # Visualization library(tidyverse) # Data manipulation, exploration and visualization","title":"Installation"},{"location":"pages/Teaching/cfr/rp5/#libraries","text":"Library Repositories Library Repository","title":"Libraries"},{"location":"pages/Teaching/cfr/rp6/","text":"Algorithm","title":"Rp6"},{"location":"pages/Teaching/cfr/rp6/#algorithm","text":"","title":"Algorithm"},{"location":"pages/Teaching/cfr/rp7/","text":"Loops","title":"Rp7"},{"location":"pages/Teaching/cfr/rp7/#loops","text":"","title":"Loops"},{"location":"pages/Teaching/cfr/rp8/","text":"Functions Calling Variables and Environment Objects Defining variables within the software environment or alternatively importing data from external resources amounts to variables to become available under the workspace for immediate use.","title":"Rp8"},{"location":"pages/Teaching/cfr/rp8/#functions","text":"Calling Variables and Environment Objects Defining variables within the software environment or alternatively importing data from external resources amounts to variables to become available under the workspace for immediate use.","title":"Functions"},{"location":"pages/Teaching/cfr/rp9/","text":"System Commands system","title":"Rp9"},{"location":"pages/Teaching/cfr/rp9/#system-commands","text":"system","title":"System Commands"},{"location":"pages/Teaching/ds/","text":"Data Science This pre-sessional course is designed to serve as preparation for empirical exercises that require you to work with several datasets and statistical methods during your studies in the forthcoming year. This is an online module providing you with the flexibility to learn the contents at your pace during the first two weeks of your enrolment in the postgraduate programmes. The course is designed to review the key concepts and methodologies covered as a standard part of several undergraduate degrees. However, there is no preliminary knowledge required and it is recommended that everyone regardless of their backgrounds thoroughly covers the course contents either as a refresher or as a basis to formally acquaint themselves with the contents for the first time. This module employs a \u201csupported learning\u201d model. In order that you to achieve the desired learning outcomes, the course provides you with access to a teaching assistant to support you through the learning materials and associated activities. The course is designed for a two-week study with approximately 40 hours to cover the contents and additional reading material. While the course remains open and accessible for you to return and review its contents, support from the teaching assistants is limited to the first two weeks. Therefore, it is highly recommended that everyone engages with the course within the specified timeline and ensures to fully utilise the available support. The course is associated with zero credit and is not considered a formal part of the assessments for your degree programme. However, you are expected to closely engage with the course material. Course Outline The course is designed to provide a preliminary knowledge of working with datasets, statistical methods and implementations in Excel which will be essential to work more productively when enrolled on semester 1 and 2 courses including the following subjects: Introduction to data types and measurements Descriptive Statistics Data Management (datafeed, retrieval and post techniques) Data-driven Specification and Validation Basic knowledge of the topics above will be expected during all semesters 1 and 2 courses and therefore it is important to cover the course material over the first two weeks to acquaint yourself with the contents: Week 1: 29 August - 4 September Week 2: 5-11 September Week 3: 12-19 September Textbook and Reading List The main textbook for the course is: Statistics for Business and Economics, Paul Newbold (8/9 Editions) Textbook Information Online Access via the library The university library provides online copies following the reference above. Hardcopies are also available from the central library. There are additional supplementary readings included in the course and accessible via both online or hardcopies from the central library: New Horizons for a Data-Driven Economy Each topic is assigned a specific reading from a section or chapter. Covering the main reading is essential to fully learn the course contents. The course contents close follow the main textbook and provide further illustrations according to the following summary notes: Summary Notes Software All empirical exercises are implemented in Excel. While many other software packages such as Stata, R, Python or Matlab are also available to carry out computational steps, the course exclusively used Microsoft Excel since this provides a benchmark skill for everyone when working with data and practical exercises. Excel also is considered an essential skill both within and outside academia and anyone who aspires to pursue a future career that involves working with data is expected to be able to carry out several computations in Excel. Review the video demonstration below to enable Excel's Data Analysis Toolbox. Course Support Course support is provided by two graduate teaching assistants: Please indicate the following information when raising a question with the team: (1) the section number or exercise number when raising questions and (2) include a part of your working case. This can be an incomplete attempt but helps the support team to provide more specific support on your questions. The support is primarily provided via the course forum or email, however, online office hours during the course timeline is possible and can be arranged.","title":"Data Science"},{"location":"pages/Teaching/ds/#data-science","text":"This pre-sessional course is designed to serve as preparation for empirical exercises that require you to work with several datasets and statistical methods during your studies in the forthcoming year. This is an online module providing you with the flexibility to learn the contents at your pace during the first two weeks of your enrolment in the postgraduate programmes. The course is designed to review the key concepts and methodologies covered as a standard part of several undergraduate degrees. However, there is no preliminary knowledge required and it is recommended that everyone regardless of their backgrounds thoroughly covers the course contents either as a refresher or as a basis to formally acquaint themselves with the contents for the first time. This module employs a \u201csupported learning\u201d model. In order that you to achieve the desired learning outcomes, the course provides you with access to a teaching assistant to support you through the learning materials and associated activities. The course is designed for a two-week study with approximately 40 hours to cover the contents and additional reading material. While the course remains open and accessible for you to return and review its contents, support from the teaching assistants is limited to the first two weeks. Therefore, it is highly recommended that everyone engages with the course within the specified timeline and ensures to fully utilise the available support. The course is associated with zero credit and is not considered a formal part of the assessments for your degree programme. However, you are expected to closely engage with the course material.","title":"Data Science"},{"location":"pages/Teaching/ds/#course-outline","text":"The course is designed to provide a preliminary knowledge of working with datasets, statistical methods and implementations in Excel which will be essential to work more productively when enrolled on semester 1 and 2 courses including the following subjects: Introduction to data types and measurements Descriptive Statistics Data Management (datafeed, retrieval and post techniques) Data-driven Specification and Validation Basic knowledge of the topics above will be expected during all semesters 1 and 2 courses and therefore it is important to cover the course material over the first two weeks to acquaint yourself with the contents: Week 1: 29 August - 4 September Week 2: 5-11 September Week 3: 12-19 September","title":"Course Outline"},{"location":"pages/Teaching/ds/#textbook-and-reading-list","text":"The main textbook for the course is: Statistics for Business and Economics, Paul Newbold (8/9 Editions) Textbook Information Online Access via the library The university library provides online copies following the reference above. Hardcopies are also available from the central library. There are additional supplementary readings included in the course and accessible via both online or hardcopies from the central library: New Horizons for a Data-Driven Economy Each topic is assigned a specific reading from a section or chapter. Covering the main reading is essential to fully learn the course contents. The course contents close follow the main textbook and provide further illustrations according to the following summary notes: Summary Notes","title":"Textbook and Reading List"},{"location":"pages/Teaching/ds/#software","text":"All empirical exercises are implemented in Excel. While many other software packages such as Stata, R, Python or Matlab are also available to carry out computational steps, the course exclusively used Microsoft Excel since this provides a benchmark skill for everyone when working with data and practical exercises. Excel also is considered an essential skill both within and outside academia and anyone who aspires to pursue a future career that involves working with data is expected to be able to carry out several computations in Excel. Review the video demonstration below to enable Excel's Data Analysis Toolbox.","title":"Software"},{"location":"pages/Teaching/ds/#course-support","text":"Course support is provided by two graduate teaching assistants: Please indicate the following information when raising a question with the team: (1) the section number or exercise number when raising questions and (2) include a part of your working case. This can be an incomplete attempt but helps the support team to provide more specific support on your questions. The support is primarily provided via the course forum or email, however, online office hours during the course timeline is possible and can be arranged.","title":"Course Support"},{"location":"pages/Teaching/ds/p1/","text":"Topic 1 1.1 1.2 1.3 1.4 Data has become the third pillar, in addition to conventional benchmarks including funding and entrepreneurial innovations, to drive success for businesses, government policymakers and many other institutions that can improve their performances based on historical and current observations to inform their future actions. More specifically, data serves two primary roles: first, to methodically document past and current events into interpretable information, and second, to formulate a basis to make predictions about possible future outcomes. An early example from anthropology Around 5000BC, humans strived just to co-exist amongst their surrounding inhabitants based on their superior intellectual ability to think and act, nonetheless their ability to record and pass down information was the key driver leading them to outgrow and thrive in their habitats. Image represents early form of human-made cave painting as an example of data record from real-world events serving an informative learning media at its time (New Scientists). Each of the aforementioned roles requires a clear understanding of data and relevant methodologies to process them. This course provides a preliminary review of the methodologies relevant to the first role which enables everyone to structure and organise data for the purpose of interpretation, visualisation and preparation for more advanced applications. You will learn about the methodologies relevant to such advanced applications in the forthcoming courses during your studies such as econometrics or other empirical methods courses. Definition of Data From a basic perspective, the concept of data is defined as a collection of numbers, text, symbols, sentiments and many other forms of rudimentary information in an original or unorganised format. This description encompasses a very wide range of observables surrounding individuals, businesses, governments and many institutions. With the advent of technology enhancing the operational performance of societies and businesses, the volume of data has been increasing at an exponential rate. Within the context of business operations and finance, we encounter data in its raw form every day including prices and quantities of investment opportunities traded, social media information, international trade, balance sheet information evolving on an everyday basis and many more. Internal and External Data Sources Data in its raw form bears no meaning and merely serves as a storage of raw facts and figures and only becomes informative when processed and structured towards a specific goal. Therefore, it is quintessential to set a well-specified goal when working with data. This goal is set first, when an individual or an institution produces data according to its operations, for example, companies store their data capturing every aspect of their operations from their early days in their life horizons through their maturity, or alternatively storing every interaction encountered with their customers, clients, suppliers and their own employees. In this sense, it\u2019s important for the company to set a clear purpose for how they intend to use such data since such goals determine how the data is gathered and stored for future applications. This has been an excessively active area within the operations of all companies around the world nearly without an exception during the past decade and becoming continuously even more prevalent. Companies have had a sizeable demand for data engineers to develop scalable and flexible platforms to monitor and store their data capturing every aspect of their operations. Companies perhaps have limited knowledge about how or when they may use such stored data but they fully appreciate the importance of evidence-based and data-driven approaches to learn from their own past performance, maintain their competitive advantages and re-adjust their decision based on all relevant aspects available to them. Reading: The Big Data Value Opportunity Based on the definition of data, it is also important to set a well-specified framework when working with data acquired from external sources since such ultimate goals substantially impact the way data can be acquired, lowering costs and improving many informational efficiencies that ultimately lead to more effective decision making for the users of the data. Navigate to the University central library database platform following the information below. The university provides access to several data platforms such as: Amadeus: Providing balance sheet and corporate operations information for the European firms Financial Analysis Made Easy (Fame): Providing balance sheet and corporate operations information for the UK firms Datastream (via Eikon): Providing corporate financials and other operational information for many countries around the world As a student and part of the university, you will have access to the platforms either directly or via creating an account on the external data platforms. Access to the Wharton Research Data Services (WRDS) is provided based on a separate account created on the WRDS platform and using your institutional email address. The account becomes active in a few days after the central library approves your request. It is recommended that you visit WRDS during the first weeks of your enrollment and activate your accounts so that it is ready to use when you start your course in semester 1. While access to the above sources is carried out via university subscriptions or possibly via a corporate research account, there are several publicly accessible additional data platforms providing various company, sector, market or country level economic and financial information including: The International Monetary Fund (IMF) World Bank: Open Data Platform Bank of England Database Federal Reserves Economic Data (FRED) The European Central Bank Data Warehouse (ECB) Bank for International Settlement (BIS)","title":"Topic 1"},{"location":"pages/Teaching/ds/p1/#topic-1","text":"1.1 1.2 1.3 1.4 Data has become the third pillar, in addition to conventional benchmarks including funding and entrepreneurial innovations, to drive success for businesses, government policymakers and many other institutions that can improve their performances based on historical and current observations to inform their future actions. More specifically, data serves two primary roles: first, to methodically document past and current events into interpretable information, and second, to formulate a basis to make predictions about possible future outcomes. An early example from anthropology Around 5000BC, humans strived just to co-exist amongst their surrounding inhabitants based on their superior intellectual ability to think and act, nonetheless their ability to record and pass down information was the key driver leading them to outgrow and thrive in their habitats. Image represents early form of human-made cave painting as an example of data record from real-world events serving an informative learning media at its time (New Scientists). Each of the aforementioned roles requires a clear understanding of data and relevant methodologies to process them. This course provides a preliminary review of the methodologies relevant to the first role which enables everyone to structure and organise data for the purpose of interpretation, visualisation and preparation for more advanced applications. You will learn about the methodologies relevant to such advanced applications in the forthcoming courses during your studies such as econometrics or other empirical methods courses.","title":"Topic 1"},{"location":"pages/Teaching/ds/p1/#definition-of-data","text":"From a basic perspective, the concept of data is defined as a collection of numbers, text, symbols, sentiments and many other forms of rudimentary information in an original or unorganised format. This description encompasses a very wide range of observables surrounding individuals, businesses, governments and many institutions. With the advent of technology enhancing the operational performance of societies and businesses, the volume of data has been increasing at an exponential rate. Within the context of business operations and finance, we encounter data in its raw form every day including prices and quantities of investment opportunities traded, social media information, international trade, balance sheet information evolving on an everyday basis and many more.","title":"Definition of Data"},{"location":"pages/Teaching/ds/p1/#internal-and-external-data-sources","text":"Data in its raw form bears no meaning and merely serves as a storage of raw facts and figures and only becomes informative when processed and structured towards a specific goal. Therefore, it is quintessential to set a well-specified goal when working with data. This goal is set first, when an individual or an institution produces data according to its operations, for example, companies store their data capturing every aspect of their operations from their early days in their life horizons through their maturity, or alternatively storing every interaction encountered with their customers, clients, suppliers and their own employees. In this sense, it\u2019s important for the company to set a clear purpose for how they intend to use such data since such goals determine how the data is gathered and stored for future applications. This has been an excessively active area within the operations of all companies around the world nearly without an exception during the past decade and becoming continuously even more prevalent. Companies have had a sizeable demand for data engineers to develop scalable and flexible platforms to monitor and store their data capturing every aspect of their operations. Companies perhaps have limited knowledge about how or when they may use such stored data but they fully appreciate the importance of evidence-based and data-driven approaches to learn from their own past performance, maintain their competitive advantages and re-adjust their decision based on all relevant aspects available to them. Reading: The Big Data Value Opportunity Based on the definition of data, it is also important to set a well-specified framework when working with data acquired from external sources since such ultimate goals substantially impact the way data can be acquired, lowering costs and improving many informational efficiencies that ultimately lead to more effective decision making for the users of the data. Navigate to the University central library database platform following the information below. The university provides access to several data platforms such as: Amadeus: Providing balance sheet and corporate operations information for the European firms Financial Analysis Made Easy (Fame): Providing balance sheet and corporate operations information for the UK firms Datastream (via Eikon): Providing corporate financials and other operational information for many countries around the world As a student and part of the university, you will have access to the platforms either directly or via creating an account on the external data platforms. Access to the Wharton Research Data Services (WRDS) is provided based on a separate account created on the WRDS platform and using your institutional email address. The account becomes active in a few days after the central library approves your request. It is recommended that you visit WRDS during the first weeks of your enrollment and activate your accounts so that it is ready to use when you start your course in semester 1. While access to the above sources is carried out via university subscriptions or possibly via a corporate research account, there are several publicly accessible additional data platforms providing various company, sector, market or country level economic and financial information including: The International Monetary Fund (IMF) World Bank: Open Data Platform Bank of England Database Federal Reserves Economic Data (FRED) The European Central Bank Data Warehouse (ECB) Bank for International Settlement (BIS)","title":"Internal and External Data Sources"},{"location":"pages/Teaching/ds/p2/","text":"Data Science 1.1 1.2 1.3 1.4 Unordered List topics From a basic perspective, the concept of data is defined as a collection of numbers, text, symbols, sentiments and many other forms of rudimentary information in an original or unorganised format. This description encompasses a very wide range of observables surrounding individuals, businesses, governments and many institutions. With the advent of technology enhancing the operational performance of societies and businesses, the volume of data has been increasing at an exponential rate. Within the context of business operations and finance, we encounter data in its raw form every day including prices and quantities of investment opportunities traded, social media information, international trade, balance sheet information evolving on an everyday basis and many more. Data in its raw form bears no meaning and merely serves as a storage of raw facts and figures and only becomes informative when processed and structured towards a specific goal. Therefore, it is quintessential to set a well-specified goal when working with data. This goal is set first, when an individual or an institution produces data according to its operations, for example, companies store their data capturing every aspect of their operations from their early days in their life horizons through their maturity, or alternatively storing every interaction encountered with their customers, clients, suppliers and their own employees. In this sense, it\u2019s important for the company to set a clear purpose for how they intend to use such data since such goals determine how the data is gathered and stored for future applications. This has been an excessively active area within the operations of all companies around the world nearly without an exception during the past decade and becoming continuously even more prevalent. Companies have had a sizeable demand for data engineers to develop scalable and flexible platforms to monitor and store their data capturing every aspect of their operations. Companies perhaps have limited knowledge about how or when they may use such stored data but they fully appreciate the importance of evidence-based and data-driven approaches to learn from their own past performance, maintain their competitive advantages and re-adjust their decision based on all relevant aspects available to them. Data has become the third pillar, in addition to conventional benchmarks including funding and entrepreneurial innovations, to drive success for businesses, government policymakers and many other institutions that can improve their performances based on historical and current observations to inform their future actions. More specifically, data serves two primary roles: first, to methodically document past and current events into interpretable information, and second, to formulate a basis to make predictions about possible future outcomes.","title":"Data Science"},{"location":"pages/Teaching/ds/p2/#data-science","text":"1.1 1.2 1.3 1.4 Unordered List topics From a basic perspective, the concept of data is defined as a collection of numbers, text, symbols, sentiments and many other forms of rudimentary information in an original or unorganised format. This description encompasses a very wide range of observables surrounding individuals, businesses, governments and many institutions. With the advent of technology enhancing the operational performance of societies and businesses, the volume of data has been increasing at an exponential rate. Within the context of business operations and finance, we encounter data in its raw form every day including prices and quantities of investment opportunities traded, social media information, international trade, balance sheet information evolving on an everyday basis and many more. Data in its raw form bears no meaning and merely serves as a storage of raw facts and figures and only becomes informative when processed and structured towards a specific goal. Therefore, it is quintessential to set a well-specified goal when working with data. This goal is set first, when an individual or an institution produces data according to its operations, for example, companies store their data capturing every aspect of their operations from their early days in their life horizons through their maturity, or alternatively storing every interaction encountered with their customers, clients, suppliers and their own employees. In this sense, it\u2019s important for the company to set a clear purpose for how they intend to use such data since such goals determine how the data is gathered and stored for future applications. This has been an excessively active area within the operations of all companies around the world nearly without an exception during the past decade and becoming continuously even more prevalent. Companies have had a sizeable demand for data engineers to develop scalable and flexible platforms to monitor and store their data capturing every aspect of their operations. Companies perhaps have limited knowledge about how or when they may use such stored data but they fully appreciate the importance of evidence-based and data-driven approaches to learn from their own past performance, maintain their competitive advantages and re-adjust their decision based on all relevant aspects available to them. Data has become the third pillar, in addition to conventional benchmarks including funding and entrepreneurial innovations, to drive success for businesses, government policymakers and many other institutions that can improve their performances based on historical and current observations to inform their future actions. More specifically, data serves two primary roles: first, to methodically document past and current events into interpretable information, and second, to formulate a basis to make predictions about possible future outcomes.","title":"Data Science"},{"location":"pages/Teaching/ds/p5/","text":"nnn","title":"P5"},{"location":"pages/Teaching/ds/p5/#nnn","text":"","title":"nnn"}]}